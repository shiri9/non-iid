{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEbuJy7penxv9+xRD0C74Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiri9/non-iid/blob/main/label_skew_statitistical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80QzDcra3UpT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access data files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_test.csv')\n",
        "\n",
        "# Define label mapping for attack categories (including all labels from train and test sets)\n",
        "attack_mapping = {\n",
        "    'normal': 0, 'neptune': 1, 'land': 1, 'back': 1, 'teardrop': 1, 'pod': 1, 'smurf': 1,\n",
        "    'ipsweep': 2, 'nmap': 2, 'portsweep': 2, 'satan': 2,\n",
        "    'mailbomb': 1, 'apache2': 1, 'processtable': 1,  # Missing DoS labels in test set\n",
        "    'phf': 3, 'multihop': 3, 'warezclient': 3, 'warezmaster': 3, 'spy': 3, 'ftp_write': 3,\n",
        "    'guess_passwd': 3, 'imap': 3,\n",
        "    'buffer_overflow': 4, 'loadmodule': 4, 'perl': 4, 'rootkit': 4,\n",
        "    # Ensure all test labels are included\n",
        "    'mscan': 2, 'saint': 2, 'snmpgetattack': 3, 'snmpguess': 3, 'xlock': 3, 'xsnoop': 3,\n",
        "    'httptunnel': 3, 'ps': 4, 'xterm': 4,\n",
        "    'sendmail': 3, 'named': 3  # Missing labels in test set\n",
        "}\n",
        "\n",
        "# Apply the label mapping\n",
        "df_train['labels'] = df_train['labels'].replace(attack_mapping)\n",
        "df_test['labels'] = df_test['labels'].replace(attack_mapping)\n",
        "\n",
        "# Verify the unique labels after mapping\n",
        "print(\"Unique labels in train set:\", df_train['labels'].unique())\n",
        "print(\"Unique labels in test set:\", df_test['labels'].unique())\n",
        "\n",
        "# Dropping the irrelevant column 'num_outbound_cmds'\n",
        "df_train = df_train.drop('num_outbound_cmds', axis=1)\n",
        "df_test = df_test.drop('num_outbound_cmds', axis=1)\n",
        "\n",
        "# Encoding categorical columns: 'protocol_type', 'service', 'flag'\n",
        "categorical_columns = ['protocol_type', 'service', 'flag']\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_train[col] = le.fit_transform(df_train[col])\n",
        "    df_test[col] = le.transform(df_test[col])  # Important: use transform for test set, not fit_transform\n",
        "\n",
        "# Scaling numerical columns\n",
        "numerical_columns = [\n",
        "    'duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate',\n",
        "    'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
        "    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_diff_host_rate', 'dst_host_rerror_rate',\n",
        "    'dst_host_srv_rerror_rate', 'hot', 'num_compromised', 'num_root'\n",
        "]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_train[numerical_columns] = scaler.fit_transform(df_train[numerical_columns])\n",
        "df_test[numerical_columns] = scaler.transform(df_test[numerical_columns])\n",
        "\n",
        "# Convert to NumPy arrays and enforce correct types for TensorFlow\n",
        "X_train = np.array(df_train.drop('labels', axis=1)).astype(np.float32)\n",
        "y_train = np.array(df_train['labels']).astype(np.int32)\n",
        "\n",
        "X_test = np.array(df_test.drop('labels', axis=1)).astype(np.float32)\n",
        "y_test = np.array(df_test['labels']).astype(np.int32)\n",
        "\n",
        "# Convert to TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
        "\n",
        "# Check dataset shapes\n",
        "print(\"Train dataset shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test dataset shape:\", X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Number of partitions (clients)\n",
        "num_partitions = 10\n",
        "\n",
        "# Define minimum number of samples to ensure visibility for each class\n",
        "min_samples_per_class = 50  # Set this to a value that ensures visibility\n",
        "\n",
        "# Get unique classes/labels in the dataset (assuming 5 classes)\n",
        "unique_labels_noniid = df_train['labels'].unique()\n",
        "num_classes = len(unique_labels_noniid)\n",
        "\n",
        "# Assign classes to each client. Adjusted for 5 classes\n",
        "# Each client will observe a subset of the available classes\n",
        "client_class_map = {\n",
        "    0: [0, 1],  # Client 1 observes classes 0 and 1\n",
        "    1: [0, 2],  # Client 2 observes classes 0 and 2\n",
        "    2: [0, 3],  # Client 3 observes classes 0 and 3\n",
        "    3: [0, 4],  # Client 4 observes classes 0 and 4\n",
        "    4: [1, 2],  # Client 5 observes classes 1 and 2\n",
        "    5: [1, 3],  # Client 6 observes classes 1 and 3\n",
        "    6: [1, 4],  # Client 7 observes classes 1 and 4\n",
        "    7: [2, 3],  # Client 8 observes classes 2 and 3\n",
        "    8: [2, 4],  # Client 9 observes classes 2 and 4\n",
        "    9: [3, 4]   # Client 10 observes classes 3 and 4\n",
        "}\n",
        "\n",
        "# Initialize list to store partitions\n",
        "data_partitions = []\n",
        "\n",
        "# For each partition (client), we only sample data from the assigned classes\n",
        "for i in range(num_partitions):\n",
        "    partition = pd.DataFrame()\n",
        "\n",
        "    # Get the classes this client is supposed to observe\n",
        "    client_classes = client_class_map[i]\n",
        "\n",
        "    # Iterate through each class assigned to the current client\n",
        "    for label in client_classes:\n",
        "        class_data = df_train[df_train['labels'] == label]\n",
        "\n",
        "        # Generate a random proportion for the current partition and class\n",
        "        proportion = np.random.uniform(0.05, 0.5)  # Random proportion between 5% and 50%\n",
        "\n",
        "        # Ensure at least `min_samples_per_class` samples are included for each class\n",
        "        num_samples = max(min_samples_per_class, int(len(class_data) * proportion))\n",
        "\n",
        "        # Randomly sample this number of instances from the class data\n",
        "        sampled_data = class_data.sample(n=num_samples, replace=False)\n",
        "\n",
        "        # Append the sampled data to the current partition\n",
        "        partition = pd.concat([partition, sampled_data])\n",
        "\n",
        "    # Shuffle the partition data and reset the index\n",
        "    partition = partition.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # Add the partition to the list\n",
        "    data_partitions.append(partition)\n",
        "\n",
        "    # Display class distribution in this partition\n",
        "    print(f\"Partition {i+1} class distribution:\")\n",
        "    print(partition['labels'].value_counts())\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "5rKUQqdCvKrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ## Cell3: Create Label-Skew Partitions\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "NUM_CLIENTS = 10\n",
        "CLASS_MAPPING = {'Benign': 0, 'DoS': 1, 'Probe': 2, 'U2R': 3, 'R2L': 4}\n",
        "MIN_SAMPLES_PER_CLASS = 50  # Prevent class starvation\n",
        "\n",
        "# Label distribution per client (matches your paper's setup)\n",
        "client_class_map = {\n",
        "    0: ['Benign', 'DoS'],\n",
        "    1: ['Benign', 'Probe'],\n",
        "    2: ['Benign', 'U2R'],\n",
        "    3: ['Benign', 'R2L'],\n",
        "    4: ['DoS', 'Probe'],\n",
        "    5: ['DoS', 'U2R'],\n",
        "    6: ['DoS', 'R2L'],\n",
        "    7: ['Probe', 'U2R'],\n",
        "    8: ['Probe', 'R2L'],\n",
        "    9: ['U2R', 'R2L']\n",
        "}\n",
        "\n",
        "data_partitions = []\n",
        "for client_id in range(NUM_CLIENTS):\n",
        "    client_partition = pd.DataFrame()\n",
        "    classes = client_class_map[client_id]\n",
        "\n",
        "    for class_name in classes:\n",
        "        label = CLASS_MAPPING[class_name]\n",
        "        class_data = df_train[df_train['labels'] == label]\n",
        "\n",
        "        # Dynamic sampling with minimum guarantee\n",
        "        proportion = np.random.uniform(0.1, 0.4)  # 10-40% of class data\n",
        "        num_samples = max(MIN_SAMPLES_PER_CLASS, int(len(class_data) * proportion))\n",
        "\n",
        "        client_partition = pd.concat([\n",
        "            client_partition,\n",
        "            class_data.sample(n=num_samples, random_state=42+client_id)\n",
        "        ])\n",
        "\n",
        "    # Shuffle and store\n",
        "    data_partitions.append(\n",
        "        client_partition.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # Verification\n",
        "    print(f\"\\nClient {client_id+1} Distribution:\")\n",
        "    print(client_partition['labels'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "RKxCZ-BWvVou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ## Cell4 : Create TensorFlow Datasets (Final Corrected Version)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Configuration\n",
        "batch_size = 32\n",
        "SEED = 42  # For reproducible shuffling\n",
        "\n",
        "train_datasets = []\n",
        "val_datasets = []\n",
        "\n",
        "for client_id, partition in enumerate(data_partitions):\n",
        "    # ========== Exact 90/10 Split ==========\n",
        "    total_samples = len(partition)\n",
        "    train_samples = (total_samples // 10) * 9  # Exact 90%\n",
        "    val_samples = total_samples - train_samples  # Exact 10%\n",
        "\n",
        "    # Shuffle with client-specific seed\n",
        "    shuffled_partition = partition.sample(frac=1, random_state=SEED+client_id).reset_index(drop=True)\n",
        "\n",
        "    # Split into train/val\n",
        "    train_part = shuffled_partition.iloc[:train_samples]\n",
        "    val_part = shuffled_partition.iloc[train_samples:]\n",
        "\n",
        "    # ========== Feature/Label Conversion ==========\n",
        "    # Training data\n",
        "    train_features = train_part.drop(columns=['labels']).values.astype(np.float32)\n",
        "    train_labels = train_part['labels'].values.astype(np.int32)\n",
        "\n",
        "    # Validation data\n",
        "    val_features = val_part.drop(columns=['labels']).values.astype(np.float32)\n",
        "    val_labels = val_part['labels'].values.astype(np.int32)\n",
        "\n",
        "    # ========== Dataset Creation ==========\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (train_features, train_labels)\n",
        "    ).batch(batch_size)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (val_features, val_labels)\n",
        "    ).batch(batch_size)\n",
        "\n",
        "    # Store datasets\n",
        "    train_datasets.append(train_dataset)\n",
        "    val_datasets.append(val_dataset)\n",
        "\n",
        "    # ========== Verification ==========\n",
        "    print(f\"Client {client_id+1}:\")\n",
        "    print(f\"  Train: {len(train_part)} samples | Classes: {np.unique(train_labels)}\")\n",
        "    print(f\"  Val: {len(val_part)} samples | Classes: {np.unique(val_labels)}\\n\")\n",
        "\n",
        "# ========== Test Dataset ==========\n",
        "test_features = df_test.drop(columns=['labels']).values.astype(np.float32)\n",
        "test_labels = df_test['labels'].values.astype(np.int32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_features, test_labels)\n",
        ").batch(batch_size)\n",
        "\n",
        "print(\"=== Final Verification ===\")\n",
        "print(f\"Total training clients: {len(train_datasets)}\")\n",
        "print(f\"Test samples: {len(test_labels)}\")\n",
        "print(f\"Test features shape: {test_features.shape}\")"
      ],
      "metadata": {
        "id": "VISmOyIqvW74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ## Cell5: Centralized Training (Label Skew Version)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def centralized_training(seed=42):\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "\n",
        "    # 1. Combine all client partitions (NEW FOR LABEL SKEW)\n",
        "    full_train = pd.concat(data_partitions).sample(frac=1, random_state=seed)\n",
        "    full_train_features = full_train.drop('labels', axis=1).values.astype(np.float32)\n",
        "    full_train_labels = full_train['labels'].values.astype(np.int32)\n",
        "\n",
        "    # 2. Create model (same architecture as FL)\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(40,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # 3. Training with metrics tracking\n",
        "    model.compile(optimizer='adam',\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(\n",
        "        full_train_features, full_train_labels,\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        validation_data=(test_features, test_labels),  # Use your existing test data\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # 4. Final evaluation\n",
        "    y_pred = np.argmax(model.predict(test_features), axis=1)\n",
        "\n",
        "    return {\n",
        "        'seed': seed,\n",
        "        'train_loss': history.history['loss'],\n",
        "        'val_loss': history.history['val_loss'],\n",
        "        'test_accuracy': history.history['val_accuracy'][-1],\n",
        "        'test_precision': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'test_recall': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'test_f1': f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Run with multiple seeds\n",
        "results_cl = [centralized_training(seed=s) for s in [42, 123, 456]]\n",
        "\n",
        "# Generate report\n",
        "report_cl = pd.DataFrame(results_cl)\n",
        "print(\"\\nCentralized Training Results (Label Skew Scenario):\")\n",
        "display(report_cl[['seed', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1']])"
      ],
      "metadata": {
        "id": "06lTg3OVvh8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell6: Federated Learning with Statistical Significance\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import collections\n",
        "from scipy import stats\n",
        "\n",
        "# ======================\n",
        "# 1. Data Preprocessing\n",
        "# ======================\n",
        "\n",
        "#Preprocessing function to handle batching with padding\n",
        "def preprocess(dataset):\n",
        "    def batch_format_fn(features, labels):\n",
        "        return collections.OrderedDict(\n",
        "            x=tf.reshape(features, [-1, 40]),  # Flatten features to match input shape\n",
        "            y=tf.reshape(labels, [-1])  # Reshape labels to match expected shape\n",
        "        )\n",
        "\n",
        "    # Pad and batch datasets\n",
        "    padded_shapes = ([None, 40], [None])  # Correct padded shapes for features and labels\n",
        "    return dataset.padded_batch(32, padded_shapes=padded_shapes).map(batch_format_fn).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 2. Core FL Functions\n",
        "# ======================\n",
        "\n",
        "def create_keras_model():\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(40,)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "def run_fl_trial(seed=42):\n",
        "    \"\"\"Run complete FL pipeline with specified seed\"\"\"\n",
        "    # Set all seeds\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Rebuild fresh training process\n",
        "    def model_fn():\n",
        "        return tff.learning.models.from_keras_model(\n",
        "            create_keras_model(),\n",
        "            input_spec=federated_train_data[0].element_spec,\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "        )\n",
        "\n",
        "    training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn,\n",
        "        client_optimizer_fn=tff.learning.optimizers.build_adam(0.001),\n",
        "        server_optimizer_fn=tff.learning.optimizers.build_adam(0.01)\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    state = training_process.initialize()\n",
        "    for _ in range(30):\n",
        "        state = training_process.next(state, federated_train_data).state\n",
        "\n",
        "    # Evaluation\n",
        "    eval_model = create_keras_model()\n",
        "    eval_model.set_weights(list(training_process.get_model_weights(state).trainable))\n",
        "    y_pred = np.argmax(eval_model.predict(test_features), axis=1)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'recall': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'f1': f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    }\n",
        "\n",
        "# ======================\n",
        "# 3. Execution & Analysis\n",
        "# ======================\n",
        "\n",
        "# First create federated data (once)\n",
        "NUM_CLIENTS = 10\n",
        "federated_train_data = make_federated_data(train_datasets, list(range(NUM_CLIENTS)))\n",
        "\n",
        "# Run multiple FL trials\n",
        "fl_seeds = [42, 123, 456]\n",
        "fl_results = [run_fl_trial(seed=s) for s in fl_seeds]\n"
      ],
      "metadata": {
        "id": "ZXWIgedTwE4_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}