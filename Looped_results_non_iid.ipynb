{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhXQLpFRQI05tFt0tXXaKR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiri9/non-iid/blob/main/Looped_results_non_iid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RfFMNKEj1C8"
      },
      "outputs": [],
      "source": [
        "# Install TensorFlow and all dependencies explicitly compatible with TFF 0.87.0\n",
        "%pip install tensorflow==2.15.0\n",
        "%pip install tensorflow-federated==0.81.0\n",
        "%pip install tensorflow-privacy==0.9.0\n",
        "%pip install tensorflow-model-optimization==0.7.5\n",
        "%pip install jax==0.4.14 jaxlib==0.4.14\n",
        "%pip install google-vizier==0.1.11\n",
        "%pip install dp-accounting==0.4.3\n",
        "%pip install portpicker==1.6.0\n",
        "%pip install scipy==1.9.3\n",
        "%pip install numpy==1.25.2\n",
        "%pip install protobuf==3.20.3\n",
        "%pip install typing-extensions==4.7.1\n",
        "%pip install googleapis-common-protos==1.61.0\n",
        "%pip install dm-tree==0.1.8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/lib/python3.11/dist-packages/jax_plugins"
      ],
      "metadata": {
        "id": "93Cna6TbkCDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"TFF version:\", tff.__version__)"
      ],
      "metadata": {
        "id": "K8XesKEzkO2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell 1: Data Loading & Preprocessing\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive to access data files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Load the raw KDD train/test CSVs\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_train.csv')\n",
        "df_test  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_test.csv')\n",
        "\n",
        "# 3. Map attack labels into 5 categories (0: normal; 1: DoS; 2: Probe; 3: U2R; 4: R2L)\n",
        "attack_mapping = {\n",
        "    'normal': 0,\n",
        "    'neptune': 1, 'land': 1, 'back': 1, 'teardrop': 1, 'pod': 1, 'smurf': 1,\n",
        "    'ipsweep': 2, 'nmap': 2, 'portsweep': 2, 'satan': 2,\n",
        "    'mailbomb': 1, 'apache2': 1, 'processtable': 1,\n",
        "    'phf': 3, 'multihop': 3, 'warezclient': 3, 'warezmaster': 3,\n",
        "    'spy': 3, 'ftp_write': 3, 'guess_passwd': 3, 'imap': 3,\n",
        "    'buffer_overflow': 4, 'loadmodule': 4, 'perl': 4, 'rootkit': 4,\n",
        "    'mscan': 2, 'saint': 2, 'snmpgetattack': 3, 'snmpguess': 3,\n",
        "    'xlock': 3, 'xsnoop': 3, 'httptunnel': 3, 'ps': 4, 'xterm': 4,\n",
        "    'sendmail': 3, 'named': 3\n",
        "}\n",
        "\n",
        "# 4. Apply the mapping\n",
        "df_train['labels'] = df_train['labels'].replace(attack_mapping)\n",
        "df_test['labels']  = df_test['labels'].replace(attack_mapping)\n",
        "\n",
        "# 5. Drop the irrelevant column 'num_outbound_cmds' if it exists\n",
        "if 'num_outbound_cmds' in df_train.columns:\n",
        "    df_train = df_train.drop('num_outbound_cmds', axis=1)\n",
        "if 'num_outbound_cmds' in df_test.columns:\n",
        "    df_test = df_test.drop('num_outbound_cmds', axis=1)\n",
        "\n",
        "# 6. Encode categorical columns: 'protocol_type', 'service', 'flag'\n",
        "categorical_columns = ['protocol_type', 'service', 'flag']\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_train[col] = le.fit_transform(df_train[col])\n",
        "    df_test[col]  = le.transform(df_test[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 7. Scale numerical columns between 0 and 1\n",
        "numerical_columns = [\n",
        "    'duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count',\n",
        "    'serror_rate', 'srv_serror_rate', 'same_srv_rate', 'dst_host_count',\n",
        "    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_serror_rate',\n",
        "    'dst_host_srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
        "    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_diff_host_rate',\n",
        "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'hot',\n",
        "    'num_compromised', 'num_root'\n",
        "]\n",
        "scaler = MinMaxScaler()\n",
        "df_train[numerical_columns] = scaler.fit_transform(df_train[numerical_columns])\n",
        "df_test[numerical_columns]  = scaler.transform(df_test[numerical_columns])\n",
        "\n",
        "# 8. Prepare test_features and test_labels for final evaluation\n",
        "X_test = df_test.drop('labels', axis=1).values.astype(np.float32)\n",
        "y_test = df_test['labels'].values.astype(np.int32)\n",
        "test_features = X_test\n",
        "test_labels   = y_test\n",
        "\n",
        "# 9. Print verification\n",
        "print(\"Unique labels in train after mapping:\", np.unique(df_train['labels']))\n",
        "print(\"Unique labels in test after mapping: \", np.unique(df_test['labels']))\n",
        "print(\"Test features shape:\", test_features.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)\n"
      ],
      "metadata": {
        "id": "b8PomHJHj3D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell 2: Dirichlet Partitioning (variable α, variable seed)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def create_partitions(alpha: float, seed: int, num_clients: int = 10, num_classes: int = 5, min_samples_per_client: int = 100):\n",
        "    \"\"\"\n",
        "    Splits df_train into `num_clients` partitions using a Dirichlet(α) label distribution.\n",
        "    Returns a list of Pandas DataFrames, one per client.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 1. Generate client-specific label probabilities via Dirichlet\n",
        "    client_label_probs = np.random.dirichlet([alpha] * num_classes, size=num_clients)\n",
        "\n",
        "    data_partitions = []\n",
        "    for client_id in range(num_clients):\n",
        "        client_probs = client_label_probs[client_id]\n",
        "        partition = pd.DataFrame()\n",
        "\n",
        "        for label in range(num_classes):\n",
        "            class_data = df_train[df_train['labels'] == label]\n",
        "            if len(class_data) == 0:\n",
        "                continue\n",
        "\n",
        "            # Guarantee at least min_samples_per_client//num_classes\n",
        "            n_min = min_samples_per_client // num_classes\n",
        "            n_dir = int(len(class_data) * client_probs[label])\n",
        "            num_samples = max(n_min, n_dir)\n",
        "            num_samples = min(num_samples, len(class_data))\n",
        "\n",
        "            sampled = class_data.sample(n=num_samples, replace=False, random_state=seed + client_id)\n",
        "            partition = pd.concat([partition, sampled], ignore_index=True)\n",
        "\n",
        "        if partition.shape[0] == 0:\n",
        "            raise ValueError(f\"Client {client_id} ended up with no data. Try smaller α or lower min_samples.\")\n",
        "\n",
        "        # Shuffle partition before returning\n",
        "        partition = partition.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "        data_partitions.append(partition)\n",
        "\n",
        "    return data_partitions\n",
        "\n",
        "# Example usage/test:\n",
        "# partitions_example = create_partitions(alpha=0.5, seed=42, num_clients=10, num_classes=5)\n",
        "# for idx, part in enumerate(partitions_example):\n",
        "#     print(f\"Client {idx} label counts:\\n{part['labels'].value_counts().sort_index()}\\n\")"
      ],
      "metadata": {
        "id": "PcYDFER7k08x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell 3: Build TensorFlow Datasets (90/10 split per client)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_client_datasets(data_partitions: list, batch_size: int = 32, seed: int = 42):\n",
        "    \"\"\"\n",
        "    Given a list of Pandas DataFrames (data_partitions), create:\n",
        "      - train_datasets: list of tf.data.Dataset for each client (90% of that client's data)\n",
        "      - val_datasets:   list of tf.data.Dataset for each client (10% of that client's data)\n",
        "    Returns: train_datasets, val_datasets (each is a length‐num_clients list).\n",
        "    \"\"\"\n",
        "    train_datasets = []\n",
        "    val_datasets   = []\n",
        "\n",
        "    for client_id, partition in enumerate(data_partitions):\n",
        "        # 1. Exact 90/10 split\n",
        "        total = len(partition)\n",
        "        n_train = (total // 10) * 9\n",
        "        n_val   = total - n_train\n",
        "\n",
        "        # Shuffle with a reproducible seed\n",
        "        shuffled = partition.sample(frac=1, random_state=seed + client_id).reset_index(drop=True)\n",
        "        df_train_part = shuffled.iloc[:n_train]\n",
        "        df_val_part   = shuffled.iloc[n_train:]\n",
        "\n",
        "        # Extract features & labels\n",
        "        X_tr = df_train_part.drop(columns=['labels']).values.astype(np.float32)\n",
        "        y_tr = df_train_part['labels'].values.astype(np.int32)\n",
        "        X_va = df_val_part.drop(columns=['labels']).values.astype(np.float32)\n",
        "        y_va = df_val_part['labels'].values.astype(np.int32)\n",
        "\n",
        "        # Create TensorFlow datasets (batched)\n",
        "        ds_train = tf.data.Dataset.from_tensor_slices((X_tr, y_tr)).shuffle(buffer_size=n_train, seed=seed+client_id).batch(batch_size)\n",
        "        ds_val   = tf.data.Dataset.from_tensor_slices((X_va, y_va)).batch(batch_size)\n",
        "\n",
        "        train_datasets.append(ds_train)\n",
        "        val_datasets.append(ds_val)\n",
        "\n",
        "    return train_datasets, val_datasets\n",
        "\n",
        "# Example usage/test:\n",
        "# data_parts = create_partitions(alpha=0.5, seed=42)\n",
        "# train_ds_list, val_ds_list = build_client_datasets(data_parts, batch_size=32, seed=42)\n",
        "# for i, ds in enumerate(train_ds_list):\n",
        "#     print(f\"Client {i} train batches:\", ds)"
      ],
      "metadata": {
        "id": "JIjLqNdllJ3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell 4: Define Two Architectures\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def build_simple_model():\n",
        "    \"\"\"\n",
        "    Simple 3-layer MLP:\n",
        "      Input(40) → Dense(128, relu) → Dense(64, relu) → Dense(5, softmax)\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(40,)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_complex_model():\n",
        "    \"\"\"\n",
        "    More complex MLP:\n",
        "      Input(40) → Dense(256, relu) → Dense(128, relu) → Dense(64, relu) → Dense(5, softmax)\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(40,)),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Quick check of parameter counts:\n",
        "# m1 = build_simple_model()\n",
        "# m2 = build_complex_model()\n",
        "# print(\"Simple params:\", m1.count_params())\n",
        "# print(\"Complex params:\", m2.count_params())"
      ],
      "metadata": {
        "id": "m0AeaxJWlOXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell 5: Federated Learning (FedAvg) with Dropout and Metric Logging\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import numpy as np\n",
        "import time\n",
        "import collections\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 5.1. Helper to convert a batched tf.data.Dataset to (X_all, y_all) NumPy arrays\n",
        "def dataset_to_numpy(dataset: tf.data.Dataset):\n",
        "    \"\"\"\n",
        "    Given a tf.data.Dataset of (features, labels), unbatch and stack into NumPy arrays.\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    for x_b, y_b in dataset.unbatch():\n",
        "        X_list.append(x_b.numpy())\n",
        "        y_list.append(y_b.numpy())\n",
        "    if len(X_list) == 0:\n",
        "        return np.zeros((0, 40), dtype=np.float32), np.zeros((0,), dtype=np.int32)\n",
        "    X_np = np.stack(X_list, axis=0)\n",
        "    y_np = np.stack(y_list, axis=0)\n",
        "    return X_np, y_np\n",
        "\n",
        "# 5.2. Main function to run FedAvg for one configuration\n",
        "def run_fedavg(\n",
        "    seed: int,\n",
        "    alpha: float,\n",
        "    dropout_rate: float,\n",
        "    architecture: str,\n",
        "    train_datasets: list,\n",
        "    val_datasets: list,\n",
        "    test_features: np.ndarray,\n",
        "    test_labels: np.ndarray,\n",
        "    num_clients: int = 10,\n",
        "    num_rounds: int = 30,\n",
        "    batch_size: int = 32\n",
        "):\n",
        "    \"\"\"\n",
        "    Executes one FedAvg experiment given:\n",
        "      - seed: random seed for reproducibility\n",
        "      - alpha: Dirichlet concentration parameter (already used upstream)\n",
        "      - dropout_rate: fraction of clients dropped each round\n",
        "      - architecture: either \"simple\" or \"complex\"\n",
        "      - train_datasets: list of tf.data.Dataset (batched) for each client\n",
        "      - val_datasets:   list of tf.data.Dataset (batched) for each client\n",
        "      - test_features/test_labels: central test set\n",
        "    Returns:\n",
        "      - global_metrics: dict of global results\n",
        "      - local_metrics:  dict mapping client_id → local result dict\n",
        "    \"\"\"\n",
        "    # 1. Set seeds\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 2. Choose model builder\n",
        "    if architecture == 'simple':\n",
        "        model_fn = build_simple_model\n",
        "    else:\n",
        "        model_fn = build_complex_model\n",
        "\n",
        "    # 3. Compute model size in MB (float32: 4 bytes per weight)\n",
        "    temp_model = model_fn()\n",
        "    model_size_bytes = sum([tf.size(w).numpy() for w in temp_model.weights]) * 4\n",
        "    model_size_mb = model_size_bytes / (1024 ** 2)\n",
        "\n",
        "    # 4. Prepare federated_train_data for all clients\n",
        "    def preprocess(dataset):\n",
        "        \"\"\" Wraps each client's tf.data.Dataset into the TFF expected format. \"\"\"\n",
        "        def batch_format_fn(features, labels):\n",
        "            return collections.OrderedDict(\n",
        "                x=tf.reshape(features, [-1, 40]),\n",
        "                y=tf.reshape(labels, [-1])\n",
        "            )\n",
        "        return dataset.map(batch_format_fn).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    federated_train_data = [preprocess(ds) for ds in train_datasets]\n",
        "\n",
        "    # 5. Define TFF model function\n",
        "    def tff_model_fn():\n",
        "        keras_model = model_fn()\n",
        "        return tff.learning.models.from_keras_model(\n",
        "            keras_model,\n",
        "            input_spec=federated_train_data[0].element_spec,\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "        )\n",
        "\n",
        "    # 6. Build the Federated Averaging process\n",
        "    iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn=tff_model_fn,\n",
        "        client_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.001),\n",
        "        server_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.01)\n",
        "    )\n",
        "\n",
        "    state = iterative_process.initialize()\n",
        "    train_losses = []            # store average client train loss each round\n",
        "    comm_cost_mb = 0.0           # total communication cost (MB)\n",
        "\n",
        "    # 7. Precompute client subsets for each round (dropout simulation)\n",
        "    clients_per_round = int(num_clients * (1.0 - dropout_rate))\n",
        "    if clients_per_round < 1:\n",
        "        raise ValueError(\"dropout_rate is too large: no clients would remain.\")\n",
        "\n",
        "    client_subsets = []\n",
        "    for r in range(num_rounds):\n",
        "        chosen = np.random.choice(\n",
        "            np.arange(num_clients),\n",
        "            size=clients_per_round,\n",
        "            replace=False\n",
        "        )\n",
        "        client_subsets.append(chosen.tolist())\n",
        "\n",
        "    # 8. Training loop\n",
        "    start_time = time.time()\n",
        "    for round_idx in range(num_rounds):\n",
        "        participating = client_subsets[round_idx]\n",
        "        fed_data = [federated_train_data[i] for i in participating]\n",
        "\n",
        "        result = iterative_process.next(state, fed_data)\n",
        "        state = result.state\n",
        "        train_loss = result.metrics['client_work']['train']['loss']\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Communication cost: each participating client downloads + uploads model once per round\n",
        "        comm_cost_mb += model_size_mb * len(participating) * 2\n",
        "\n",
        "        # (Optional) Print intermediate metrics every 5 rounds\n",
        "        if (round_idx + 1) % 5 == 0:\n",
        "            # Evaluate current global model on central test set\n",
        "            eval_model = model_fn()\n",
        "            eval_model.set_weights(iterative_process.get_model_weights(state).trainable)\n",
        "            logits = eval_model.predict(test_features, batch_size=batch_size)\n",
        "            y_pred = np.argmax(logits, axis=1)\n",
        "            acc  = accuracy_score(test_labels, y_pred)\n",
        "            prec = precision_score(test_labels, y_pred, average='macro', zero_division=0)\n",
        "            rec  = recall_score(test_labels, y_pred, average='macro', zero_division=0)\n",
        "            f1   = f1_score(test_labels, y_pred, average='macro', zero_division=0)\n",
        "            test_loss = tf.keras.losses.sparse_categorical_crossentropy(test_labels, logits).numpy().mean()\n",
        "\n",
        "            print(f\"\\n[Round {round_idx+1}/{num_rounds}] \"\n",
        "                  f\"Client‐avg Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | \"\n",
        "                  f\"Acc: {acc:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # 9. Final global evaluation on central test set\n",
        "    final_model = model_fn()\n",
        "    final_model.set_weights(iterative_process.get_model_weights(state).trainable)\n",
        "    final_logits = final_model.predict(test_features, batch_size=batch_size)\n",
        "    final_pred   = np.argmax(final_logits, axis=1)\n",
        "    final_test_loss  = tf.keras.losses.sparse_categorical_crossentropy(test_labels, final_logits).numpy().mean()\n",
        "    final_acc        = accuracy_score(test_labels, final_pred)\n",
        "    final_prec       = precision_score(test_labels, final_pred, average='macro', zero_division=0)\n",
        "    final_rec        = recall_score(test_labels, final_pred, average='macro', zero_division=0)\n",
        "    final_f1         = f1_score(test_labels, final_pred, average='macro', zero_division=0)\n",
        "\n",
        "    global_metrics = {\n",
        "        'seed': seed,\n",
        "        'alpha': alpha,\n",
        "        'dropout_rate': dropout_rate,\n",
        "        'architecture': architecture,\n",
        "        'train_time_sec': train_time,\n",
        "        'model_size_mb': model_size_mb,\n",
        "        'comm_cost_mb': comm_cost_mb,\n",
        "        'train_loss_curve': train_losses,        # list of length num_rounds\n",
        "        'final_test_loss': final_test_loss,\n",
        "        'accuracy': final_acc,\n",
        "        'precision': final_prec,\n",
        "        'recall': final_rec,\n",
        "        'f1_score': final_f1\n",
        "    }\n",
        "\n",
        "    # 10. Local evaluation: apply the final global model on each client's validation set\n",
        "    local_metrics = {}\n",
        "    for client_id, val_ds in enumerate(val_datasets):\n",
        "        X_val, y_val = dataset_to_numpy(val_ds)\n",
        "        if X_val.shape[0] == 0:\n",
        "            # If a client had zero validation samples, record NaNs\n",
        "            local_metrics[client_id] = {\n",
        "                'accuracy': np.nan,\n",
        "                'precision': np.nan,\n",
        "                'recall': np.nan,\n",
        "                'f1_score': np.nan\n",
        "            }\n",
        "            continue\n",
        "\n",
        "        y_pred_loc = np.argmax(final_model.predict(X_val, batch_size=batch_size), axis=1)\n",
        "        acc_loc  = accuracy_score(y_val, y_pred_loc)\n",
        "        prec_loc = precision_score(y_val, y_pred_loc, average='macro', zero_division=0)\n",
        "        rec_loc  = recall_score(y_val, y_pred_loc, average='macro', zero_division=0)\n",
        "        f1_loc   = f1_score(y_val, y_pred_loc, average='macro', zero_division=0)\n",
        "\n",
        "        local_metrics[client_id] = {\n",
        "            'accuracy': acc_loc,\n",
        "            'precision': prec_loc,\n",
        "            'recall': rec_loc,\n",
        "            'f1_score': f1_loc\n",
        "        }\n",
        "\n",
        "    return global_metrics, local_metrics\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# (You can test run one configuration, e.g.:\n",
        "# gm, lm = run_fedavg(seed=42, alpha=0.5, dropout_rate=0.1,\n",
        "#                    architecture='simple',\n",
        "#                    train_datasets=train_ds_list,\n",
        "#                    val_datasets=val_ds_list,\n",
        "#                    test_features=test_features, test_labels=test_labels)\n",
        "# print(\"Global metrics:\\n\", gm)\n",
        "# print(\"Local metrics:\\n\", lm)\n",
        "# )\n",
        "# ----------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "xeDh4M7XlZvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell 6: Execute Experiments and Collect Results\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 6.1. Define hyperparameter grid\n",
        "seeds        = [42, 123, 456]\n",
        "alpha_list   = [0.1, 0.5, 1.0, 5.0]        # Dirichlet α values\n",
        "dropout_list = [0.0, 0.1, 0.2]             # Fraction of clients dropped each round\n",
        "architectures = ['simple', 'complex']      # Two model variants\n",
        "\n",
        "# 6.2. Initialize containers\n",
        "all_global_results = []    # each entry: dict with global metrics + metadata\n",
        "all_local_results  = []    # each entry: dict with local metrics + metadata\n",
        "\n",
        "# 6.3. Loop over every combination\n",
        "for seed in seeds:\n",
        "    for alpha in alpha_list:\n",
        "        # (a) Create a fresh set of Dirichlet partitions\n",
        "        partitions = create_partitions(alpha=alpha, seed=seed, num_clients=10, num_classes=5, min_samples_per_client=100)\n",
        "\n",
        "        # (b) Build train/val datasets for each client\n",
        "        train_ds_list, val_ds_list = build_client_datasets(data_partitions=partitions, batch_size=32, seed=seed)\n",
        "\n",
        "        for dropout_rate in dropout_list:\n",
        "            for arch in architectures:\n",
        "                print(f\"\\n=== Running: seed={seed} | alpha={alpha} | dropout={dropout_rate} | arch={arch} ===\")\n",
        "\n",
        "                # (c) Run FedAvg for this condition\n",
        "                g_metrics, l_metrics = run_fedavg(\n",
        "                    seed=seed,\n",
        "                    alpha=alpha,\n",
        "                    dropout_rate=dropout_rate,\n",
        "                    architecture=arch,\n",
        "                    train_datasets=train_ds_list,\n",
        "                    val_datasets=val_ds_list,\n",
        "                    test_features=test_features,\n",
        "                    test_labels=test_labels,\n",
        "                    num_clients=10,\n",
        "                    num_rounds=30,\n",
        "                    batch_size=32\n",
        "                )\n",
        "\n",
        "                # (d) Append global metrics\n",
        "                all_global_results.append(g_metrics)\n",
        "\n",
        "                # (e) Append local metrics (one entry per client_id)\n",
        "                for client_id, metrics_dict in l_metrics.items():\n",
        "                    row = {\n",
        "                        'seed': seed,\n",
        "                        'alpha': alpha,\n",
        "                        'dropout_rate': dropout_rate,\n",
        "                        'architecture': arch,\n",
        "                        'client_id': client_id,\n",
        "                        'local_accuracy': metrics_dict['accuracy'],\n",
        "                        'local_precision': metrics_dict['precision'],\n",
        "                        'local_recall': metrics_dict['recall'],\n",
        "                        'local_f1_score': metrics_dict['f1_score']\n",
        "                    }\n",
        "                    all_local_results.append(row)\n",
        "\n",
        "# 6.4. Convert to pandas DataFrames\n",
        "\n",
        "# 6.4.1. Global results DataFrame\n",
        "df_global_results = pd.DataFrame(all_global_results)\n",
        "\n",
        "# Expand train_loss_curve into separate columns if desired, or store as list\n",
        "# For simplicity, we leave 'train_loss_curve' as a list column. You can drop it if not needed:\n",
        "# df_global_results = df_global_results.drop(columns=['train_loss_curve'])\n",
        "\n",
        "# 6.4.2. Local results DataFrame\n",
        "df_local_results = pd.DataFrame(all_local_results)\n",
        "\n",
        "# 6.5. Display summaries\n",
        "print(\"\\n--- Global Results Summary ---\")\n",
        "display(df_global_results.head())\n",
        "\n",
        "print(\"\\n--- Local Results Summary ---\")\n",
        "display(df_local_results.head())\n",
        "\n",
        "# 6.6. Optionally, save to CSV in Drive\n",
        "df_global_results.to_csv('/content/drive/MyDrive/Colab Notebooks/fedavg_global_results.csv', index=False)\n",
        "df_local_results.to_csv('/content/drive/MyDrive/Colab Notebooks/fedavg_local_results.csv', index=False)\n",
        "\n",
        "print(\"\\nResults saved to your Google Drive under:\")\n",
        "print(\"  • fedavg_global_results.csv\")\n",
        "print(\"  • fedavg_local_results.csv\")\n"
      ],
      "metadata": {
        "id": "y8d3EUs6lbH3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}