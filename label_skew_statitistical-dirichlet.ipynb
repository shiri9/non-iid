{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOI6nJ1BHtu8CugeR+cEd9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiri9/non-iid/blob/main/label_skew_statitistical-dirichlet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow and all dependencies explicitly compatible with TFF 0.87.0\n",
        "%pip install tensorflow==2.15.0\n",
        "%pip install tensorflow-federated==0.81.0\n",
        "%pip install tensorflow-privacy==0.9.0\n",
        "%pip install tensorflow-model-optimization==0.7.5\n",
        "%pip install jax==0.4.14 jaxlib==0.4.14\n",
        "%pip install google-vizier==0.1.11\n",
        "%pip install dp-accounting==0.4.3\n",
        "%pip install portpicker==1.6.0\n",
        "%pip install scipy==1.9.3\n",
        "%pip install numpy==1.25.2\n",
        "%pip install protobuf==3.20.3\n",
        "%pip install typing-extensions==4.7.1\n",
        "%pip install googleapis-common-protos==1.61.0\n",
        "%pip install dm-tree==0.1.8"
      ],
      "metadata": {
        "id": "Gz8B4VFI9q3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5gJVBJWLpy1",
        "outputId": "b4fae325-27e1-4db8-bfe0-acc9f124c1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/lib/python3.11/dist-packages/jax_plugins"
      ],
      "metadata": {
        "id": "ByDwyo9EOOYb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"TFF version:\", tff.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f8zXU4R6JmA",
        "outputId": "3b6d2943-4d2c-4148-fe67-6077ef4e2f82"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.14.1\n",
            "TFF version: 0.81.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "80QzDcra3UpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bf594e-50e2-457e-cebf-1c0c8ca5a7d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-43537eeb4cce>:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_train['labels'] = df_train['labels'].replace(attack_mapping)\n",
            "<ipython-input-11-43537eeb4cce>:31: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_test['labels'] = df_test['labels'].replace(attack_mapping)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels in train set: [0 1 3 2 4]\n",
            "Unique labels in test set: [0 2 1 3 4]\n",
            "Train dataset shape: (125973, 40) (125973,)\n",
            "Test dataset shape: (22544, 40) (22544,)\n"
          ]
        }
      ],
      "source": [
        "#cell 1\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access data files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_test.csv')\n",
        "\n",
        "# Define label mapping for attack categories (including all labels from train and test sets)\n",
        "attack_mapping = {\n",
        "    'normal': 0, 'neptune': 1, 'land': 1, 'back': 1, 'teardrop': 1, 'pod': 1, 'smurf': 1,\n",
        "    'ipsweep': 2, 'nmap': 2, 'portsweep': 2, 'satan': 2,\n",
        "    'mailbomb': 1, 'apache2': 1, 'processtable': 1,  # Missing DoS labels in test set\n",
        "    'phf': 3, 'multihop': 3, 'warezclient': 3, 'warezmaster': 3, 'spy': 3, 'ftp_write': 3,\n",
        "    'guess_passwd': 3, 'imap': 3,\n",
        "    'buffer_overflow': 4, 'loadmodule': 4, 'perl': 4, 'rootkit': 4,\n",
        "    # Ensure all test labels are included\n",
        "    'mscan': 2, 'saint': 2, 'snmpgetattack': 3, 'snmpguess': 3, 'xlock': 3, 'xsnoop': 3,\n",
        "    'httptunnel': 3, 'ps': 4, 'xterm': 4,\n",
        "    'sendmail': 3, 'named': 3  # Missing labels in test set\n",
        "}\n",
        "\n",
        "# Apply the label mapping\n",
        "df_train['labels'] = df_train['labels'].replace(attack_mapping)\n",
        "df_test['labels'] = df_test['labels'].replace(attack_mapping)\n",
        "\n",
        "# Verify the unique labels after mapping\n",
        "print(\"Unique labels in train set:\", df_train['labels'].unique())\n",
        "print(\"Unique labels in test set:\", df_test['labels'].unique())\n",
        "\n",
        "# Dropping the irrelevant column 'num_outbound_cmds'\n",
        "df_train = df_train.drop('num_outbound_cmds', axis=1)\n",
        "df_test = df_test.drop('num_outbound_cmds', axis=1)\n",
        "\n",
        "# Encoding categorical columns: 'protocol_type', 'service', 'flag'\n",
        "categorical_columns = ['protocol_type', 'service', 'flag']\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_train[col] = le.fit_transform(df_train[col])\n",
        "    df_test[col] = le.transform(df_test[col])  # Important: use transform for test set, not fit_transform\n",
        "\n",
        "# Scaling numerical columns\n",
        "numerical_columns = [\n",
        "    'duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate',\n",
        "    'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
        "    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_diff_host_rate', 'dst_host_rerror_rate',\n",
        "    'dst_host_srv_rerror_rate', 'hot', 'num_compromised', 'num_root'\n",
        "]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_train[numerical_columns] = scaler.fit_transform(df_train[numerical_columns])\n",
        "df_test[numerical_columns] = scaler.transform(df_test[numerical_columns])\n",
        "\n",
        "# Convert to NumPy arrays and enforce correct types for TensorFlow\n",
        "X_train = np.array(df_train.drop('labels', axis=1)).astype(np.float32)\n",
        "y_train = np.array(df_train['labels']).astype(np.int32)\n",
        "\n",
        "X_test = np.array(df_test.drop('labels', axis=1)).astype(np.float32)\n",
        "y_test = np.array(df_test['labels']).astype(np.int32)\n",
        "\n",
        "# Convert to TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
        "\n",
        "# Check dataset shapes\n",
        "print(\"Train dataset shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test dataset shape:\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell2: Create Non-IID Partitions with Dirichlet Label Skew (Modified Version)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "NUM_CLIENTS = 10\n",
        "NUM_CLASSES = 5  # Based on your 5 attack categories (0-4)\n",
        "ALPHA = 0.5      # Dirichlet concentration parameter (Î±=0.5 for moderate skew)\n",
        "SEED = 42        # For reproducibility\n",
        "MIN_SAMPLES_PER_CLIENT = 100  # Ensure clients have sufficient data\n",
        "\n",
        "np.random.seed(SEED)  # Fix randomness for reproducibility\n",
        "\n",
        "# Step 1: Generate client-specific label distributions using Dirichlet\n",
        "client_label_probs = np.random.dirichlet([ALPHA]*NUM_CLASSES, size=NUM_CLIENTS)\n",
        "\n",
        "# Initialize list to store partitions\n",
        "data_partitions = []\n",
        "\n",
        "# Create partitions using Dirichlet distributions\n",
        "for client_id in range(NUM_CLIENTS):\n",
        "    partition = pd.DataFrame()\n",
        "    client_probs = client_label_probs[client_id]  # Probability vector for this client\n",
        "\n",
        "    # For each class, sample data proportionally to Dirichlet probabilities\n",
        "    for label in range(NUM_CLASSES):\n",
        "        class_data = df_train[df_train['labels'] == label]\n",
        "        if len(class_data) == 0:\n",
        "            continue  # Skip empty classes\n",
        "\n",
        "        # Calculate number of samples for this class\n",
        "        num_samples = max(\n",
        "            MIN_SAMPLES_PER_CLIENT // NUM_CLASSES,  # Minimum guarantee\n",
        "            int(len(class_data) * client_probs[label])  # Dirichlet proportion\n",
        "        )\n",
        "\n",
        "        # Sample without replacement\n",
        "        sampled_data = class_data.sample(n=num_samples, replace=False, random_state=SEED+client_id)\n",
        "        partition = pd.concat([partition, sampled_data])\n",
        "\n",
        "    # Final checks\n",
        "    if len(partition) == 0:\n",
        "        raise ValueError(f\"Client {client_id} has no data!\")\n",
        "\n",
        "    # Shuffle and store\n",
        "    data_partitions.append(\n",
        "        partition.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # Print verification\n",
        "    print(f\"\\nClient {client_id+1} Label Distribution (Dirichlet Î±={ALPHA}):\")\n",
        "    print(partition['labels'].value_counts().sort_index())\n",
        "    print(f\"Target distribution: {np.round(client_probs, 2)}\")\n",
        "\n",
        "print(\"\\nSuccessfully created Dirichlet-based non-IID partitions!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rKUQqdCvKrZ",
        "outputId": "eb0a0253-3d07-48ef-a9fa-60ee61261ac9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Client 1 Label Distribution (Dirichlet Î±=0.5):\n",
            "labels\n",
            "0     7854\n",
            "1    25164\n",
            "2      235\n",
            "3       20\n",
            "4       20\n",
            "Name: count, dtype: int64\n",
            "Target distribution: [0.12 0.55 0.02 0.   0.31]\n",
            "\n",
            "Client 2 Label Distribution (Dirichlet Î±=0.5):\n",
            "labels\n",
            "0       20\n",
            "1    35730\n",
            "2      273\n",
            "3       65\n",
            "4       20\n",
            "Name: count, dtype: int64\n",
            "Target distribution: [0.   0.78 0.02 0.07 0.13]\n",
            "\n",
            "Client 3 Label Distribution (Dirichlet Î±=0.5):\n",
            "labels\n",
            "0    23786\n",
            "1     3526\n",
            "2     2181\n",
            "3       35\n",
            "4       20\n",
            "Name: count, dtype: int64\n",
            "Target distribution: [0.35 0.08 0.19 0.04 0.35]\n",
            "\n",
            "Client 4 Label Distribution (Dirichlet Î±=0.5):\n",
            "labels\n",
            "0       70\n",
            "1    38573\n",
            "2      268\n",
            "3      131\n",
            "4       20\n",
            "Name: count, dtype: int64\n",
            "Target distribution: [0.   0.84 0.02 0.13 0.  ]\n",
            "\n",
            "Client 5 Label Distribution (Dirichlet Î±=0.5):\n",
            "labels\n",
            "0     20\n",
            "1    754\n",
            "2    277\n",
            "3     73\n",
            "4     46\n",
            "Name: count, dtype: int64\n",
            "Target distribution: [0.   0.02 0.02 0.07 0.89]\n",
            "\n",
            "Client 6 Label Distribution (Dirichlet Î±=0.5):\n",
            "labels\n",
            "0    55215\n",
            "1     5767\n",
            "2       30\n",
            "3       20\n",
            "4       20\n",
            "Name: count, dtype: int64\n",
            "Target distribution: [0.82 0.13 0.   0.   0.05]\n",
            "\n",
            "Client 7 Label Distribution (Dirichlet Î±=0.5):\n",
            "labels\n",
            "0    36580\n",
            "1     1835\n",
            "2      117\n",
            "3       20\n",
            "4       21\n",
            "Name: count, dtype: int64\n",
            "Target distribution: [0.54 0.04 0.01 0.   0.4 ]\n",
            "\n",
            "Client 8 Label Distribution (Dirichlet Î±=0.5):\n",
            "labels\n",
            "0      20\n",
            "1    9662\n",
            "2    3309\n",
            "3     468\n",
            "4      20\n",
            "Name: count, dtype: int64\n",
            "Target distribution: [0.   0.21 0.28 0.47 0.03]\n",
            "\n",
            "Client 9 Label Distribution (Dirichlet Î±=0.5):\n",
            "labels\n",
            "0    11667\n",
            "1    18907\n",
            "2       44\n",
            "3      199\n",
            "4       20\n",
            "Name: count, dtype: int64\n",
            "Target distribution: [0.17 0.41 0.   0.2  0.21]\n",
            "\n",
            "Client 10 Label Distribution (Dirichlet Î±=0.5):\n",
            "labels\n",
            "0    8455\n",
            "1      20\n",
            "2      20\n",
            "3      45\n",
            "4      43\n",
            "Name: count, dtype: int64\n",
            "Target distribution: [0.13 0.   0.   0.05 0.83]\n",
            "\n",
            "Successfully created Dirichlet-based non-IID partitions!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ## Cell3: Create Label-Skew Partitions\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "NUM_CLIENTS = 10\n",
        "CLASS_MAPPING = {'Benign': 0, 'DoS': 1, 'Probe': 2, 'U2R': 3, 'R2L': 4}\n",
        "MIN_SAMPLES_PER_CLASS = 50  # Prevent class starvation\n",
        "\n",
        "# Label distribution per client (matches your paper's setup)\n",
        "client_class_map = {\n",
        "    0: ['Benign', 'DoS'],\n",
        "    1: ['Benign', 'Probe'],\n",
        "    2: ['Benign', 'U2R'],\n",
        "    3: ['Benign', 'R2L'],\n",
        "    4: ['DoS', 'Probe'],\n",
        "    5: ['DoS', 'U2R'],\n",
        "    6: ['DoS', 'R2L'],\n",
        "    7: ['Probe', 'U2R'],\n",
        "    8: ['Probe', 'R2L'],\n",
        "    9: ['U2R', 'R2L']\n",
        "}\n",
        "\n",
        "data_partitions = []\n",
        "for client_id in range(NUM_CLIENTS):\n",
        "    client_partition = pd.DataFrame()\n",
        "    classes = client_class_map[client_id]\n",
        "\n",
        "    for class_name in classes:\n",
        "        label = CLASS_MAPPING[class_name]\n",
        "        class_data = df_train[df_train['labels'] == label]\n",
        "\n",
        "        # Dynamic sampling with minimum guarantee\n",
        "        proportion = np.random.uniform(0.1, 0.4)  # 10-40% of class data\n",
        "        num_samples = max(MIN_SAMPLES_PER_CLASS, int(len(class_data) * proportion))\n",
        "\n",
        "        client_partition = pd.concat([\n",
        "            client_partition,\n",
        "            class_data.sample(n=num_samples, random_state=42+client_id)\n",
        "        ])\n",
        "\n",
        "    # Shuffle and store\n",
        "    data_partitions.append(\n",
        "        client_partition.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # Verification\n",
        "    print(f\"\\nClient {client_id+1} Distribution:\")\n",
        "    print(client_partition['labels'].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKxCZ-BWvVou",
        "outputId": "68031228-6ae4-4130-843d-476e1afeeeb7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Client 1 Distribution:\n",
            "labels\n",
            "0    15025\n",
            "1    15002\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Client 2 Distribution:\n",
            "labels\n",
            "0    11356\n",
            "2     1434\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Client 3 Distribution:\n",
            "labels\n",
            "0    12588\n",
            "3      147\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Client 4 Distribution:\n",
            "labels\n",
            "0    25516\n",
            "4       50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Client 5 Distribution:\n",
            "labels\n",
            "1    13319\n",
            "2     4212\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Client 6 Distribution:\n",
            "labels\n",
            "1    15665\n",
            "3      155\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Client 7 Distribution:\n",
            "labels\n",
            "1    16890\n",
            "4       50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Client 8 Distribution:\n",
            "labels\n",
            "2    3989\n",
            "3     366\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Client 9 Distribution:\n",
            "labels\n",
            "2    2277\n",
            "4      50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Client 10 Distribution:\n",
            "labels\n",
            "3    167\n",
            "4     50\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ## Cell4 : Create TensorFlow Datasets (Final Corrected Version)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Configuration\n",
        "batch_size = 32\n",
        "SEED = 42  # For reproducible shuffling\n",
        "\n",
        "train_datasets = []\n",
        "val_datasets = []\n",
        "\n",
        "for client_id, partition in enumerate(data_partitions):\n",
        "    # ========== Exact 90/10 Split ==========\n",
        "    total_samples = len(partition)\n",
        "    train_samples = (total_samples // 10) * 9  # Exact 90%\n",
        "    val_samples = total_samples - train_samples  # Exact 10%\n",
        "\n",
        "    # Shuffle with client-specific seed\n",
        "    shuffled_partition = partition.sample(frac=1, random_state=SEED+client_id).reset_index(drop=True)\n",
        "\n",
        "    # Split into train/val\n",
        "    train_part = shuffled_partition.iloc[:train_samples]\n",
        "    val_part = shuffled_partition.iloc[train_samples:]\n",
        "\n",
        "    # ========== Feature/Label Conversion ==========\n",
        "    # Training data\n",
        "    train_features = train_part.drop(columns=['labels']).values.astype(np.float32)\n",
        "    train_labels = train_part['labels'].values.astype(np.int32)\n",
        "\n",
        "    # Validation data\n",
        "    val_features = val_part.drop(columns=['labels']).values.astype(np.float32)\n",
        "    val_labels = val_part['labels'].values.astype(np.int32)\n",
        "\n",
        "    # ========== Dataset Creation ==========\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (train_features, train_labels)\n",
        "    ).batch(batch_size)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (val_features, val_labels)\n",
        "    ).batch(batch_size)\n",
        "\n",
        "    # Store datasets\n",
        "    train_datasets.append(train_dataset)\n",
        "    val_datasets.append(val_dataset)\n",
        "\n",
        "    # ========== Verification ==========\n",
        "    print(f\"Client {client_id+1}:\")\n",
        "    print(f\"  Train: {len(train_part)} samples | Classes: {np.unique(train_labels)}\")\n",
        "    print(f\"  Val: {len(val_part)} samples | Classes: {np.unique(val_labels)}\\n\")\n",
        "\n",
        "# ========== Test Dataset ==========\n",
        "test_features = df_test.drop(columns=['labels']).values.astype(np.float32)\n",
        "test_labels = df_test['labels'].values.astype(np.int32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_features, test_labels)\n",
        ").batch(batch_size)\n",
        "\n",
        "print(\"=== Final Verification ===\")\n",
        "print(f\"Total training clients: {len(train_datasets)}\")\n",
        "print(f\"Test samples: {len(test_labels)}\")\n",
        "print(f\"Test features shape: {test_features.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VISmOyIqvW74",
        "outputId": "7aeeac79-df31-49ea-f3a7-0ac15fb36053"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1:\n",
            "  Train: 27018 samples | Classes: [0 1]\n",
            "  Val: 3009 samples | Classes: [0 1]\n",
            "\n",
            "Client 2:\n",
            "  Train: 11511 samples | Classes: [0 2]\n",
            "  Val: 1279 samples | Classes: [0 2]\n",
            "\n",
            "Client 3:\n",
            "  Train: 11457 samples | Classes: [0 3]\n",
            "  Val: 1278 samples | Classes: [0 3]\n",
            "\n",
            "Client 4:\n",
            "  Train: 23004 samples | Classes: [0 4]\n",
            "  Val: 2562 samples | Classes: [0 4]\n",
            "\n",
            "Client 5:\n",
            "  Train: 15777 samples | Classes: [1 2]\n",
            "  Val: 1754 samples | Classes: [1 2]\n",
            "\n",
            "Client 6:\n",
            "  Train: 14238 samples | Classes: [1 3]\n",
            "  Val: 1582 samples | Classes: [1 3]\n",
            "\n",
            "Client 7:\n",
            "  Train: 15246 samples | Classes: [1 4]\n",
            "  Val: 1694 samples | Classes: [1 4]\n",
            "\n",
            "Client 8:\n",
            "  Train: 3915 samples | Classes: [2 3]\n",
            "  Val: 440 samples | Classes: [2 3]\n",
            "\n",
            "Client 9:\n",
            "  Train: 2088 samples | Classes: [2 4]\n",
            "  Val: 239 samples | Classes: [2 4]\n",
            "\n",
            "Client 10:\n",
            "  Train: 189 samples | Classes: [3 4]\n",
            "  Val: 28 samples | Classes: [3 4]\n",
            "\n",
            "=== Final Verification ===\n",
            "Total training clients: 10\n",
            "Test samples: 22544\n",
            "Test features shape: (22544, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ## Cell5: Centralized Training (Label Skew Version)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def centralized_training(seed=42):\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "\n",
        "    # 1. Combine all client partitions (NEW FOR LABEL SKEW)\n",
        "    full_train = pd.concat(data_partitions).sample(frac=1, random_state=seed)\n",
        "    full_train_features = full_train.drop('labels', axis=1).values.astype(np.float32)\n",
        "    full_train_labels = full_train['labels'].values.astype(np.int32)\n",
        "\n",
        "    # 2. Create model (same architecture as FL)\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(40,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # 3. Training with metrics tracking\n",
        "    model.compile(optimizer='adam',\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(\n",
        "        full_train_features, full_train_labels,\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        validation_data=(test_features, test_labels),  # Use your existing test data\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # 4. Final evaluation\n",
        "    y_pred = np.argmax(model.predict(test_features), axis=1)\n",
        "\n",
        "    return {\n",
        "        'seed': seed,\n",
        "        'train_loss': history.history['loss'],\n",
        "        'val_loss': history.history['val_loss'],\n",
        "        'test_accuracy': history.history['val_accuracy'][-1],\n",
        "        'test_precision': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'test_recall': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'test_f1': f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    }\n",
        "\n",
        "# Run with multiple seeds\n",
        "results_cl = [centralized_training(seed=s) for s in [42, 123, 456]]\n",
        "\n",
        "# Generate report\n",
        "report_cl = pd.DataFrame(results_cl)\n",
        "print(\"\\nCentralized Training Results (Label Skew Scenario):\")\n",
        "display(report_cl[['seed', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "06lTg3OVvh8L",
        "outputId": "40cc7560-2493-4da7-86d4-fd6660ffc307"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "705/705 [==============================] - 1s 1ms/step\n",
            "705/705 [==============================] - 1s 1ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "\n",
            "Centralized Training Results (Label Skew Scenario):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   seed  test_accuracy  test_precision  test_recall   test_f1\n",
              "0    42       0.929915        0.953275     0.670655  0.728932\n",
              "1   123       0.924814        0.855777     0.692183  0.717540\n",
              "2   456       0.933463        0.911781     0.724816  0.769453"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a9bead9-792f-4655-8559-16d4cd8e2a98\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>seed</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>test_precision</th>\n",
              "      <th>test_recall</th>\n",
              "      <th>test_f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42</td>\n",
              "      <td>0.929915</td>\n",
              "      <td>0.953275</td>\n",
              "      <td>0.670655</td>\n",
              "      <td>0.728932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>123</td>\n",
              "      <td>0.924814</td>\n",
              "      <td>0.855777</td>\n",
              "      <td>0.692183</td>\n",
              "      <td>0.717540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>456</td>\n",
              "      <td>0.933463</td>\n",
              "      <td>0.911781</td>\n",
              "      <td>0.724816</td>\n",
              "      <td>0.769453</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a9bead9-792f-4655-8559-16d4cd8e2a98')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1a9bead9-792f-4655-8559-16d4cd8e2a98 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1a9bead9-792f-4655-8559-16d4cd8e2a98');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c1452019-305e-4f9a-a646-475222e0c1ea\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c1452019-305e-4f9a-a646-475222e0c1ea')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c1452019-305e-4f9a-a646-475222e0c1ea button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(report_cl[['seed', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1']])\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"seed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 219,\n        \"min\": 42,\n        \"max\": 456,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          42,\n          123,\n          456\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00434804269944619,\n        \"min\": 0.9248136878013611,\n        \"max\": 0.933463454246521,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9299148321151733,\n          0.9248136878013611,\n          0.933463454246521\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.048928556352541244,\n        \"min\": 0.8557768857175289,\n        \"max\": 0.9532747255363543,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9532747255363543,\n          0.8557768857175289,\n          0.9117811004678428\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02726950366364693,\n        \"min\": 0.6706554590019773,\n        \"max\": 0.7248161854346515,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.6706554590019773,\n          0.6921825101976615,\n          0.7248161854346515\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.027284679535199766,\n        \"min\": 0.7175401194275619,\n        \"max\": 0.7694531093361618,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7289315080973939,\n          0.7175401194275619,\n          0.7694531093361618\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell6: Federated Learning with Statistical Significance\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import collections\n",
        "from scipy import stats\n",
        "\n",
        "# ======================\n",
        "# 1. Data Preprocessing\n",
        "# ======================\n",
        "\n",
        "def preprocess(dataset):\n",
        "    def batch_format_fn(features, labels):\n",
        "        return collections.OrderedDict(\n",
        "            x=tf.reshape(features, [-1, 40]),  # Flatten features\n",
        "            y=tf.reshape(labels, [-1])  # Reshape labels\n",
        "        )\n",
        "    padded_shapes = ([None, 40], [None])\n",
        "    return dataset.padded_batch(32, padded_shapes=padded_shapes).map(batch_format_fn).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# ======================\n",
        "# 2. Core FL Functions\n",
        "# ======================\n",
        "\n",
        "def create_keras_model():\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(40,)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "def make_federated_data(client_data, client_ids):\n",
        "    \"\"\"Global function to create federated datasets\"\"\"\n",
        "    return [\n",
        "        preprocess(client_data[i])\n",
        "        for i in client_ids\n",
        "        if len(list(client_data[i])) > 0  # Filter empty datasets\n",
        "    ]\n",
        "\n",
        "def run_fl_trial(seed=42, num_rounds=30):\n",
        "    \"\"\"Run complete FL pipeline with specified seed\"\"\"\n",
        "    # Set all seeds\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Model function\n",
        "    def model_fn():\n",
        "        return tff.learning.models.from_keras_model(\n",
        "            create_keras_model(),\n",
        "            input_spec=federated_train_data[0].element_spec,\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "        )\n",
        "\n",
        "    # Build training process\n",
        "    training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn,\n",
        "        client_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.001),\n",
        "        server_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.01)\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    state = training_process.initialize()\n",
        "    for _ in range(num_rounds):\n",
        "        state = training_process.next(state, federated_train_data).state\n",
        "\n",
        "    # Evaluation\n",
        "    eval_model = create_keras_model()\n",
        "    eval_model.set_weights(list(training_process.get_model_weights(state).trainable))\n",
        "    y_pred = np.argmax(eval_model.predict(test_features), axis=1)\n",
        "\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'precision': precision_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'recall': recall_score(y_test, y_pred, average='macro', zero_division=0),\n",
        "        'f1': f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    }\n",
        "\n",
        "# ======================\n",
        "# 3. Execution & Analysis\n",
        "# ======================\n",
        "\n",
        "# Configuration\n",
        "NUM_CLIENTS = 10\n",
        "SEEDS = [42, 123, 456]\n",
        "\n",
        "# Create federated data (ensure train_datasets exists)\n",
        "federated_train_data = make_federated_data(train_datasets, list(range(NUM_CLIENTS)))\n",
        "\n",
        "# Run trials\n",
        "fl_results = [run_fl_trial(seed=s) for s in SEEDS]\n",
        "\n",
        "# Statistical comparison with centralized results (assuming results_cl exists)\n",
        "def print_stat_comparison(cl_results, fl_results):\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "        cl_values = [r[f'test_{metric}'] for r in cl_results]\n",
        "        fl_values = [r[metric] for r in fl_results]\n",
        "        t_stat, p_value = stats.ttest_ind(cl_values, fl_values)\n",
        "\n",
        "        print(f\"\\n{metric.upper():<10} CL: {np.mean(cl_values):.4f} Â± {np.std(cl_values):.4f}\")\n",
        "        print(f\"{'FL:':<10} {np.mean(fl_values):.4f} Â± {np.std(fl_values):.4f}\")\n",
        "        print(f\"{'p-value:':<10} {p_value:.4e}{'*' if p_value < 0.05 else ''}\")\n",
        "\n",
        "print(\"\\n=== Statistical Significance ===\")\n",
        "print_stat_comparison(results_cl, fl_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXWIgedTwE4_",
        "outputId": "d0688b52-6e8f-4442-b16e-bcc411559a95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "705/705 [==============================] - 1s 1ms/step\n",
            "705/705 [==============================] - 2s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "\n",
            "=== Statistical Significance ===\n",
            "\n",
            "ACCURACY   CL: 0.9294 Â± 0.0036\n",
            "FL:        0.8395 Â± 0.0114\n",
            "p-value:   4.3965e-04*\n",
            "\n",
            "PRECISION  CL: 0.9069 Â± 0.0399\n",
            "FL:        0.4738 Â± 0.0906\n",
            "p-value:   3.4658e-03*\n",
            "\n",
            "RECALL     CL: 0.6959 Â± 0.0223\n",
            "FL:        0.4063 Â± 0.0186\n",
            "p-value:   1.4659e-04*\n",
            "\n",
            "F1         CL: 0.7386 Â± 0.0223\n",
            "FL:        0.3991 Â± 0.0293\n",
            "p-value:   1.9983e-04*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "unique_labels, counts = np.unique(y_test, return_counts=True)\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"Class {label}: {count} samples ({count/len(y_test):.1%})\")"
      ],
      "metadata": {
        "id": "Aw0fvuKdElec",
        "outputId": "fecd3062-a573-4aaf-f6ed-78bc7cb92ae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: 11245 samples (49.9%)\n",
            "Class 1: 8095 samples (35.9%)\n",
            "Class 2: 2157 samples (9.6%)\n",
            "Class 3: 1009 samples (4.5%)\n",
            "Class 4: 38 samples (0.2%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell5: Enhanced Centralized Training with Full Metrics\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def centralized_training(seed=42, epochs=30):\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "\n",
        "    # 1. Combine all client data\n",
        "    full_train = pd.concat(data_partitions).sample(frac=1, random_state=seed)\n",
        "    X_train = full_train.drop('labels', axis=1).values.astype(np.float32)\n",
        "    y_train = full_train['labels'].values.astype(np.int32)\n",
        "\n",
        "    # 2. Model definition\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(40,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 3. Custom callback for metrics\n",
        "    class CLMetrics(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            # Test set evaluation\n",
        "            y_pred = np.argmax(model.predict(test_features), axis=1)\n",
        "            logs['test_precision'] = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "            logs['test_recall'] = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "            logs['test_f1'] = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "            # Interval reporting\n",
        "            if (epoch+1) % 5 == 0:\n",
        "                print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
        "                print(f\"Train Loss: {logs['loss']:.4f} | Test Loss: {logs['val_loss']:.4f}\")\n",
        "                print(f\"Accuracy: {logs['val_accuracy']:.4f} | F1: {logs['test_f1']:.4f}\")\n",
        "\n",
        "    # 4. Training\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=32,\n",
        "        validation_data=(test_features, test_labels),\n",
        "        callbacks=[CLMetrics()],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # 5. Calculate RMSE\n",
        "    train_rmse = np.sqrt(np.mean(np.square(history.history['loss'])))\n",
        "    test_rmse = np.sqrt(np.mean(np.square(history.history['val_loss'])))\n",
        "\n",
        "    return {\n",
        "        'train_loss': history.history['loss'],\n",
        "        'test_loss': history.history['val_loss'],\n",
        "        'test_accuracy': history.history['val_accuracy'],\n",
        "        'test_precision': [history.history[f'test_precision'][i] for i in range(epochs)],\n",
        "        'test_recall': [history.history[f'test_recall'][i] for i in range(epochs)],\n",
        "        'test_f1': [history.history[f'test_f1'][i] for i in range(epochs)],\n",
        "        'train_rmse': train_rmse,\n",
        "        'test_rmse': test_rmse\n",
        "    }\n",
        "\n",
        "# Run centralized training\n",
        "cl_results = centralized_training()\n",
        "print(f\"\\nFinal RMSE - Train: {cl_results['train_rmse']:.4f}, Test: {cl_results['test_rmse']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_CkQF3mO9AN",
        "outputId": "3ddf605c-0eee-4783-ede5-e48965ce9c4f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "705/705 [==============================] - 2s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "\n",
            "Epoch 5/30:\n",
            "Train Loss: 0.0260 | Test Loss: 0.7114\n",
            "Accuracy: 0.9142 | F1: 0.7121\n",
            "705/705 [==============================] - 2s 2ms/step\n",
            "705/705 [==============================] - 2s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 1ms/step\n",
            "\n",
            "Epoch 10/30:\n",
            "Train Loss: 0.0190 | Test Loss: 0.7805\n",
            "Accuracy: 0.9269 | F1: 0.7333\n",
            "705/705 [==============================] - 2s 2ms/step\n",
            "705/705 [==============================] - 1s 1ms/step\n",
            "705/705 [==============================] - 1s 1ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 2s 2ms/step\n",
            "\n",
            "Epoch 15/30:\n",
            "Train Loss: 0.0148 | Test Loss: 0.8244\n",
            "Accuracy: 0.9291 | F1: 0.7439\n",
            "705/705 [==============================] - 2s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "\n",
            "Epoch 20/30:\n",
            "Train Loss: 0.0129 | Test Loss: 1.0537\n",
            "Accuracy: 0.9299 | F1: 0.7547\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 2s 2ms/step\n",
            "705/705 [==============================] - 2s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "\n",
            "Epoch 25/30:\n",
            "Train Loss: 0.0124 | Test Loss: 1.0186\n",
            "Accuracy: 0.9316 | F1: 0.7400\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 2s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "705/705 [==============================] - 1s 2ms/step\n",
            "\n",
            "Epoch 30/30:\n",
            "Train Loss: 0.0118 | Test Loss: 1.2363\n",
            "Accuracy: 0.9299 | F1: 0.7289\n",
            "\n",
            "Final RMSE - Train: 0.0269, Test: 0.8993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell6: Federated Learning with Client Metrics (Fixed)\n",
        "import tensorflow_federated as tff\n",
        "import time\n",
        "\n",
        "def run_fl_trial(seed=42, num_rounds=30):\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 1. Initialize TFF process\n",
        "    def model_fn():\n",
        "        keras_model = create_keras_model()\n",
        "        return tff.learning.models.from_keras_model(\n",
        "            keras_model,\n",
        "            input_spec=federated_train_data[0].element_spec,\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "        )\n",
        "\n",
        "    training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn,\n",
        "        client_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.001),\n",
        "        server_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.01)\n",
        "    )\n",
        "\n",
        "    state = training_process.initialize()\n",
        "\n",
        "    # 2. Metric containers\n",
        "    metrics = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "        'test_prec': [],\n",
        "        'test_rec': [],\n",
        "        'test_f1': [],\n",
        "        'client_metrics': {cid: [] for cid in range(NUM_CLIENTS)}\n",
        "    }\n",
        "\n",
        "    # 3. TFF-compatible evaluation function\n",
        "    @tf.function\n",
        "    def evaluate_model(model, dataset):\n",
        "        metric = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
        "        acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "        for batch in dataset:\n",
        "            preds = model(batch['x'])\n",
        "            metric.update_state(batch['y'], preds)\n",
        "            acc_metric.update_state(batch['y'], preds)\n",
        "        return metric.result().numpy(), acc_metric.result().numpy()\n",
        "\n",
        "    # 4. Training loop\n",
        "    for round_num in range(num_rounds):\n",
        "        # 4a. Train one round\n",
        "        result = training_process.next(state, federated_train_data)\n",
        "        state = result.state\n",
        "        metrics['train_loss'].append(result.metrics['client_work']['train']['loss'])\n",
        "\n",
        "        # 4b. Global evaluation\n",
        "        global_model = training_process.get_model_weights(state)\n",
        "        keras_model = create_keras_model()\n",
        "        keras_model.set_weights(global_model)\n",
        "\n",
        "        # Convert test data to TFF-compatible format\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "            collections.OrderedDict(x=test_features, y=test_labels)\n",
        "        ).batch(32)\n",
        "        test_loss, test_acc = evaluate_model(keras_model, test_dataset)\n",
        "\n",
        "        y_pred = np.argmax(keras_model.predict(test_features), axis=1)\n",
        "        metrics['test_loss'].append(test_loss)\n",
        "        metrics['test_acc'].append(test_acc)\n",
        "        metrics['test_prec'].append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
        "        metrics['test_rec'].append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n",
        "        metrics['test_f1'].append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n",
        "\n",
        "        # 4c. Per-client evaluation\n",
        "        for cid in range(NUM_CLIENTS):\n",
        "            client_dataset = val_datasets[cid]\n",
        "            client_loss, client_acc = evaluate_model(keras_model, client_dataset)\n",
        "\n",
        "            y_pred_client = np.argmax(keras_model.predict(client_dataset), axis=1)\n",
        "            y_true_client = np.concatenate([y for _, y in client_dataset])\n",
        "\n",
        "            metrics['client_metrics'][cid].append({\n",
        "                'accuracy': client_acc,\n",
        "                'precision': precision_score(y_true_client, y_pred_client, average='macro', zero_division=0),\n",
        "                'recall': recall_score(y_true_client, y_pred_client, average='macro', zero_division=0),\n",
        "                'f1': f1_score(y_true_client, y_pred_client, average='macro', zero_division=0)\n",
        "            })\n",
        "\n",
        "        # 4d. Interval reporting\n",
        "        if (round_num+1) % 5 == 0:\n",
        "            print(f\"\\nRound {round_num+1}/{num_rounds}:\")\n",
        "            print(f\"Global Test Loss: {metrics['test_loss'][-1]:.4f}\")\n",
        "            print(f\"Global Accuracy: {metrics['test_acc'][-1]:.4f}\")\n",
        "            print(f\"Client 0 F1: {metrics['client_metrics'][0][-1]['f1']:.4f}\")\n",
        "\n",
        "    # 5. Calculate RMSE\n",
        "    metrics['train_rmse'] = np.sqrt(np.mean(np.square(metrics['train_loss']))\n",
        "    metrics['test_rmse'] = np.sqrt(np.mean(np.square(metrics['test_loss']))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Execute federated training\n",
        "fl_results = run_fl_trial()\n",
        "print(f\"\\nFinal RMSE: Train={fl_results['train_rmse']:.4f}, Test={fl_results['test_rmse']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "-bUhuasnPDz4",
        "outputId": "004c2688-8a40-49b5-cf89-2069c2cfdfe8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "resource: Attempting to capture an EagerTensor without building a function.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-29684fe33d01>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Run federated learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mfl_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_fl_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nFinal RMSE - Train: {fl_results['train_rmse']:.4f}, Test: {fl_results['test_rmse']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-29684fe33d01>\u001b[0m in \u001b[0;36mrun_fl_trial\u001b[0;34m(seed, num_rounds)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 1. Initialize model and process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     training_process = tff.learning.algorithms.build_weighted_fed_avg(\n\u001b[0m\u001b[1;32m     12\u001b[0m         lambda: tff.learning.models.from_keras_model(\n\u001b[1;32m     13\u001b[0m             \u001b[0mkeras_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/learning/algorithms/fed_avg.py\u001b[0m in \u001b[0;36mbuild_weighted_fed_avg\u001b[0;34m(model_fn, client_optimizer_fn, server_optimizer_fn, client_weighting, model_distributor, model_aggregator, metrics_aggregator, use_experimental_simulation_loop)\u001b[0m\n\u001b[1;32m    207\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mtensorflow_computation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_computation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitial_model_weights_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# \"success\" case below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m       \u001b[0mprovided_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m       return lambda fn: _wrap(\n\u001b[0m\u001b[1;32m    496\u001b[0m           \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapper_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovided_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_type_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py\u001b[0m in \u001b[0;36m_wrap\u001b[0;34m(fn, wrapper_fn, parameter_types, infer_type_fn)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# Either we have a concrete parameter type, or this is no-arg function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mparameter_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parameter_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mwrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_concrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m   \u001b[0;31m# When applying a decorator, the __doc__ attribute with the documentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py\u001b[0m in \u001b[0;36m_wrap_concrete\u001b[0;34m(fn, wrapper_fn, parameter_type)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0mconcrete_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m   py_typecheck.check_type(\n\u001b[1;32m    100\u001b[0m       \u001b[0mconcrete_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/environments/tensorflow_frontend/tensorflow_computation.py\u001b[0m in \u001b[0;36m_tf_wrapper_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0mcontext_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_stack_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   comp_pb, extra_type_spec = (\n\u001b[0;32m---> 65\u001b[0;31m       tensorflow_serialization.serialize_py_fn_as_tf_computation(\n\u001b[0m\u001b[1;32m     66\u001b[0m           \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/environments/tensorflow_frontend/tensorflow_serialization.py\u001b[0m in \u001b[0;36mserialize_py_fn_as_tf_computation\u001b[0;34m(fn, parameter_type, context_stack)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0minit_op_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     result_type, result_binding = tensorflow_utils.capture_result_from_graph(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py\u001b[0m in \u001b[0;36mcapture_result_from_graph\u001b[0;34m(result, graph)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;31m# failing to retain the information about naming of tuple members.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0mname_value_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_bindings_for_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_value_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mname_value_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py\u001b[0m in \u001b[0;36m_get_bindings_for_elements\u001b[0;34m(name_value_pairs, graph, container_type)\u001b[0m\n\u001b[1;32m    225\u001b[0m   ) -> tuple[computation_types.Type, pb.TensorFlow.Binding]:\n\u001b[1;32m    226\u001b[0m     \u001b[0;34m\"\"\"Build `(type_spec, binding)` tuple for name value pairs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     element_name_type_binding_triples = [\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcapture_result_from_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_value_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;34m\"\"\"Build `(type_spec, binding)` tuple for name value pairs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     element_name_type_binding_triples = [\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcapture_result_from_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname_value_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py\u001b[0m in \u001b[0;36mcapture_result_from_graph\u001b[0;34m(result, graph)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_bindings_for_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_value_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     element_type_binding_pairs = [\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0mcapture_result_from_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    326\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     element_type_binding_pairs = [\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mcapture_result_from_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m     ]\n\u001b[1;32m    330\u001b[0m     return (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow_federated/python/core/impl/utils/tensorflow_utils.py\u001b[0m in \u001b[0;36mcapture_result_from_graph\u001b[0;34m(result, graph)\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;31m# We have a tf.Variable-like result, get a proper tensor to fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;31m# Otherwise we insert an identity. TensorFlow does not allow the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    812\u001b[0m     \"\"\"\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    791\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_and_set_handle\u001b[0;34m(no_copy)\u001b[0m\n\u001b[1;32m    781\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mno_copy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforward_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_copy_on_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m       result = gen_resource_variable_ops.read_variable_op(\n\u001b[0m\u001b[1;32m    784\u001b[0m           self.handle, self._dtype)\n\u001b[1;32m    785\u001b[0m       \u001b[0m_maybe_set_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    545\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m    548\u001b[0m         \"ReadVariableOp\", resource=resource, dtype=dtype, name=name)\n\u001b[1;32m    549\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    776\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m       _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n\u001b[0m\u001b[1;32m    779\u001b[0m                              \u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_type_attr_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m                              input_types)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    549\u001b[0m                 preferred_dtype=default_dtype)\n\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m           values = ops.convert_to_tensor(\n\u001b[0m\u001b[1;32m    552\u001b[0m               \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m               \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    696\u001b[0m   \u001b[0;31m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    699\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    207\u001b[0m   \u001b[0moverload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__tf_tensor__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moverload\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moverload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#  pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversion_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    585\u001b[0m       \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilding_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    588\u001b[0m             _add_error_prefix(\n\u001b[1;32m    589\u001b[0m                 \u001b[0;34m\"Attempting to capture an EagerTensor without \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: resource: Attempting to capture an EagerTensor without building a function."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Cell7: Visualization of Results\n",
        "def plot_results(cl_res, fl_res):\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Loss curves\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(cl_res['train_loss'], label='CL Train')\n",
        "    plt.plot(cl_res['test_loss'], label='CL Test')\n",
        "    plt.plot(fl_res['train_loss'], '--', label='FL Train')\n",
        "    plt.plot(fl_res['test_loss'], '--', label='FL Test')\n",
        "    plt.title('Loss Curves')\n",
        "    plt.xlabel('Rounds/Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy/F1 comparison\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(cl_res['test_accuracy'], label='CL Accuracy')\n",
        "    plt.plot(cl_res['test_f1'], label='CL F1')\n",
        "    plt.plot(fl_res['test_acc'], '--', label='FL Accuracy')\n",
        "    plt.plot(fl_res['test_f1'], '--', label='FL F1')\n",
        "    plt.title('Performance Metrics')\n",
        "    plt.xlabel('Rounds/Epochs')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate plots\n",
        "plot_results(cl_results, fl_results)"
      ],
      "metadata": {
        "id": "CuhYyIHSbeXJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}