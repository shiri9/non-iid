{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1KgAatqRTg5oudK0ICLA9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiri9/non-iid/blob/main/iid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhuR-GlJV4Hx"
      },
      "outputs": [],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision matplotlib;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "#from torchvision.datasets import MNIST\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Dataset, Subset, random_split\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import Metrics\n",
        "from flwr.common.typing import NDArrays, Scalar\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")"
      ],
      "metadata": {
        "id": "2cQQ0MdEWA0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0VBTsh02WB1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "\n",
        "\n",
        "#reading training csv file from google drive\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_test.csv')\n",
        "\n",
        "#creating copies of datasets\n",
        "df_train_copy = df_train.copy()\n",
        "df_test_copy = df_test.copy()\n",
        "\n",
        "#Encoding categories in 0-Normal, 1-DoS, 2-Probe, 3-R2L, 4-U2R\n",
        "df1_train_labels = df_train_copy['labels']\n",
        "df1_train_labels_in_numbers = df1_train_labels.replace({ 'normal' : 0, 'neptune' : 1,'land' : 1, 'back': 1, 'teardrop': 1, 'pod': 1, 'smurf' : 1,\n",
        "                                                     'ipsweep' : 2, 'nmap' : 2, 'portsweep' : 2, 'satan' : 2,\n",
        "                                                     'phf': 3, 'multihop': 3, 'warezclient': 3,'warezmaster': 3, 'spy': 3, 'ftp_write' : 3,\n",
        "                                                     'guess_passwd': 3,'imap': 3,\n",
        "                                                     'buffer_overflow': 4, 'loadmodule': 4,'perl': 4,  'rootkit': 4 })\n",
        "#replacing the string output clsses by numbers\n",
        "df_train_copy['labels'] = df1_train_labels_in_numbers\n",
        "\n",
        "#Encoding categories in 0-Normal, 1-DoS, 2-Probe, 3-R2L, 4-U2R\n",
        "df1_test_labels = df_test_copy['labels']\n",
        "df1_test_labels_in_numbers = df1_test_labels.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1,\n",
        "                                                      'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1,\n",
        "                                                     'ipsweep' : 2, 'nmap' : 2, 'portsweep' : 2, 'satan' : 2, 'mscan' : 2,'saint' : 2,\n",
        "                                                     'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3, 'warezclient': 3,\n",
        "                                                       'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,\n",
        "                                                       'xsnoop': 3,'httptunnel': 3,\n",
        "                                                     'buffer_overflow': 4, 'loadmodule': 4,'perl': 4,  'rootkit': 4, 'ps': 4,'xterm': 4 })\n",
        "#replacing the string output clsses by numbers\n",
        "df_test_copy['labels'] = df1_test_labels_in_numbers\n",
        "\n",
        "#Transform categorical features into numbers using LabelEncoder() from train dataset\n",
        "dft = df_train_copy.apply(LabelEncoder().fit_transform)\n",
        "df_train_copy = dft\n",
        "\n",
        "#Transform categorical features into numbers using LabelEncoder() from test dataset\n",
        "dftt = df_test_copy.apply(LabelEncoder().fit_transform)\n",
        "df_test_copy = dftt\n",
        "\n",
        "# Dropping the column \"num_outbound_cmds\" from train and test datasets\n",
        "df_train_copy = df_train_copy.drop('num_outbound_cmds', axis=1)\n",
        "df_test_copy = df_test_copy.drop('num_outbound_cmds', axis=1)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Select the columns to be normalized (assuming they are numerical features)\n",
        "numerical_columns = ['duration', 'service', 'flag', 'src_bytes', 'dst_bytes', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate',\n",
        "                     'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                     'dst_host_same_src_port_rate','dst_host_serror_rate', 'dst_host_srv_serror_rate', 'rerror_rate',\n",
        "                     'srv_rerror_rate','diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_diff_host_rate',\n",
        "                     'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','hot','num_compromised','num_root',]\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the selected columns\n",
        "scaler.fit(df_train_copy[numerical_columns])\n",
        "\n",
        "# Transform the selected columns with the scaler\n",
        "df_train_copy[numerical_columns] = scaler.transform(df_train_copy[numerical_columns])\n",
        "\n",
        "# Select the columns to be normalized (assuming they are numerical features)\n",
        "numerical_columns = ['duration', 'service', 'flag', 'src_bytes', 'dst_bytes', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate',\n",
        "                     'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                     'dst_host_same_src_port_rate','dst_host_serror_rate', 'dst_host_srv_serror_rate', 'rerror_rate',\n",
        "                     'srv_rerror_rate','diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_diff_host_rate',\n",
        "                     'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','hot','num_compromised','num_root',]\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the selected columns\n",
        "scaler.fit(df_test_copy[numerical_columns])\n",
        "\n",
        "# Transform the selected columns with the scaler\n",
        "df_test_copy[numerical_columns] = scaler.transform(df_test_copy[numerical_columns])"
      ],
      "metadata": {
        "id": "URdU851MWFPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iid-scenario\n",
        "\n",
        "# Determine the number of partitions\n",
        "num_partitions = 10\n",
        "\n",
        "# Split your DataFrame into 10 partitions\n",
        "partition_size_iid = len(df_train_copy) // num_partitions\n",
        "data_partitions_iid = []\n",
        "\n",
        "for i in range(num_partitions):\n",
        "    start_index = i * partition_size_iid\n",
        "    end_index = (i + 1) * partition_size_iid if i < num_partitions - 1 else len(df_train_copy)\n",
        "    partition = df_train_copy.iloc[start_index:end_index]\n",
        "    data_partitions_iid.append(partition)"
      ],
      "metadata": {
        "id": "ja_9lko7WLSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and display label counts for each partition\n",
        "for idx, partition in enumerate(data_partitions_iid):\n",
        "    label_counts = partition['labels'].value_counts()\n",
        "    print(f\"Partition {idx + 1} label counts:\")\n",
        "    print(label_counts)\n",
        "    print()"
      ],
      "metadata": {
        "id": "PKKnh4N1WN9N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}