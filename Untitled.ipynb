{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIXPde/iToiqrkl1VjrrAa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiri9/non-iid/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4tgfw2ldPFW",
        "outputId": "f517c36b-1c26-435d-ad5b-9cc8a812c4f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Unique labels in train set: [0 1 3 2 4]\n",
            "Unique labels in test set: [0 2 1 3 4]\n",
            "Train dataset shape: (125973, 40) (125973,)\n",
            "Test dataset shape: (22544, 40) (22544,)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access data files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load datasets\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/kdd_test.csv')\n",
        "\n",
        "# Define label mapping for attack categories (including all labels from train and test sets)\n",
        "attack_mapping = {\n",
        "    'normal': 0, 'neptune': 1, 'land': 1, 'back': 1, 'teardrop': 1, 'pod': 1, 'smurf': 1,\n",
        "    'ipsweep': 2, 'nmap': 2, 'portsweep': 2, 'satan': 2,\n",
        "    'mailbomb': 1, 'apache2': 1, 'processtable': 1,  # Missing DoS labels in test set\n",
        "    'phf': 3, 'multihop': 3, 'warezclient': 3, 'warezmaster': 3, 'spy': 3, 'ftp_write': 3,\n",
        "    'guess_passwd': 3, 'imap': 3,\n",
        "    'buffer_overflow': 4, 'loadmodule': 4, 'perl': 4, 'rootkit': 4,\n",
        "    # Ensure all test labels are included\n",
        "    'mscan': 2, 'saint': 2, 'snmpgetattack': 3, 'snmpguess': 3, 'xlock': 3, 'xsnoop': 3,\n",
        "    'httptunnel': 3, 'ps': 4, 'xterm': 4,\n",
        "    'sendmail': 3, 'named': 3  # Missing labels in test set\n",
        "}\n",
        "\n",
        "# Apply the label mapping\n",
        "df_train['labels'] = df_train['labels'].replace(attack_mapping)\n",
        "df_test['labels'] = df_test['labels'].replace(attack_mapping)\n",
        "\n",
        "# Verify the unique labels after mapping\n",
        "print(\"Unique labels in train set:\", df_train['labels'].unique())\n",
        "print(\"Unique labels in test set:\", df_test['labels'].unique())\n",
        "\n",
        "# Dropping the irrelevant column 'num_outbound_cmds'\n",
        "df_train = df_train.drop('num_outbound_cmds', axis=1)\n",
        "df_test = df_test.drop('num_outbound_cmds', axis=1)\n",
        "\n",
        "# Encoding categorical columns: 'protocol_type', 'service', 'flag'\n",
        "categorical_columns = ['protocol_type', 'service', 'flag']\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_train[col] = le.fit_transform(df_train[col])\n",
        "    df_test[col] = le.transform(df_test[col])  # Important: use transform for test set, not fit_transform\n",
        "\n",
        "# Scaling numerical columns\n",
        "numerical_columns = [\n",
        "    'duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate',\n",
        "    'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
        "    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_diff_host_rate', 'dst_host_rerror_rate',\n",
        "    'dst_host_srv_rerror_rate', 'hot', 'num_compromised', 'num_root'\n",
        "]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_train[numerical_columns] = scaler.fit_transform(df_train[numerical_columns])\n",
        "df_test[numerical_columns] = scaler.transform(df_test[numerical_columns])\n",
        "\n",
        "# Convert to NumPy arrays and enforce correct types for TensorFlow\n",
        "X_train = np.array(df_train.drop('labels', axis=1)).astype(np.float32)\n",
        "y_train = np.array(df_train['labels']).astype(np.int32)\n",
        "\n",
        "X_test = np.array(df_test.drop('labels', axis=1)).astype(np.float32)\n",
        "y_test = np.array(df_test['labels']).astype(np.int32)\n",
        "\n",
        "# Convert to TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
        "\n",
        "# Check dataset shapes\n",
        "print(\"Train dataset shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test dataset shape:\", X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Number of partitions (clients)\n",
        "num_partitions = 10\n",
        "\n",
        "# Get unique classes/labels in the dataset\n",
        "unique_labels_noniid = df_train['labels'].unique()\n",
        "num_classes = len(unique_labels_noniid)\n",
        "\n",
        "# Initialize list to store partitions\n",
        "data_partitions = []\n",
        "\n",
        "# For each partition, define how much of each class's data to include\n",
        "for i in range(num_partitions):\n",
        "    partition = pd.DataFrame()\n",
        "\n",
        "    # Iterate through each class\n",
        "    for label in unique_labels_noniid:\n",
        "        class_data = df_train[df_train['labels'] == label]\n",
        "\n",
        "        # Generate random proportion for the current partition and class\n",
        "        proportion = np.random.uniform(0.01, 0.3)  # Random proportion between 1% and 30%\n",
        "\n",
        "        # Ensure at least one sample is included for each class\n",
        "        num_samples = max(1, int(len(class_data) * proportion))\n",
        "\n",
        "        # Randomly sample this number of instances from the class data\n",
        "        sampled_data = class_data.sample(n=num_samples, replace=False)\n",
        "\n",
        "        # Append the sampled data to the current partition\n",
        "        partition = pd.concat([partition, sampled_data])\n",
        "\n",
        "    # Shuffle the partition data and reset index\n",
        "    partition = partition.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # Add the partition to the list\n",
        "    data_partitions.append(partition)\n",
        "\n",
        "    # Display class distribution in this partition\n",
        "    print(f\"Partition {i+1} class distribution:\")\n",
        "    print(partition['labels'].value_counts())\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvuy011bjytB",
        "outputId": "92511d5a-edda-444a-cc0d-a00e71b92459"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partition 1 class distribution:\n",
            "labels\n",
            "0    4216\n",
            "1    3644\n",
            "2    3300\n",
            "3     186\n",
            "4      12\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Partition 2 class distribution:\n",
            "labels\n",
            "0    17777\n",
            "1     3631\n",
            "2      640\n",
            "3      225\n",
            "4        6\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Partition 3 class distribution:\n",
            "labels\n",
            "1    6167\n",
            "2    2228\n",
            "0    2175\n",
            "3     282\n",
            "4      10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Partition 4 class distribution:\n",
            "labels\n",
            "0    16023\n",
            "1    10327\n",
            "2     1126\n",
            "3      293\n",
            "4        3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Partition 5 class distribution:\n",
            "labels\n",
            "0    7215\n",
            "1    3959\n",
            "2    2367\n",
            "3      13\n",
            "4      12\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Partition 6 class distribution:\n",
            "labels\n",
            "1    4740\n",
            "0    3775\n",
            "2    2366\n",
            "3     263\n",
            "4      12\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Partition 7 class distribution:\n",
            "labels\n",
            "0    11766\n",
            "1     6889\n",
            "2      751\n",
            "3      172\n",
            "4       12\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Partition 8 class distribution:\n",
            "labels\n",
            "0    15911\n",
            "1     2459\n",
            "2     1625\n",
            "3       41\n",
            "4        2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Partition 9 class distribution:\n",
            "labels\n",
            "0    3782\n",
            "2    3201\n",
            "1    1643\n",
            "3      52\n",
            "4      10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Partition 10 class distribution:\n",
            "labels\n",
            "0    6420\n",
            "2    2665\n",
            "1    1020\n",
            "3     109\n",
            "4       1\n",
            "Name: count, dtype: int64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow Datasets for each partition with validation datasets\n",
        "batch_size = 32  # Set your desired batch size\n",
        "train_datasets = []\n",
        "val_datasets = []\n",
        "\n",
        "for partition in data_partitions:\n",
        "    # Split each partition into training and validation subsets (90% train, 10% validation)\n",
        "    len_partition = len(partition)\n",
        "    len_train = int(0.9 * len_partition)\n",
        "\n",
        "    # Shuffle the partition\n",
        "    partition = partition.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Split the partition into training and validation\n",
        "    train_partition = partition.iloc[:len_train]\n",
        "    val_partition = partition.iloc[len_train:]\n",
        "\n",
        "    # Extract features and labels\n",
        "    train_features = train_partition.drop(columns=['labels']).values.astype(np.float32)\n",
        "    train_labels = train_partition['labels'].values.astype(np.int32)\n",
        "\n",
        "    val_features = val_partition.drop(columns=['labels']).values.astype(np.float32)\n",
        "    val_labels = val_partition['labels'].values.astype(np.int32)\n",
        "\n",
        "    # Create TensorFlow datasets\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).batch(batch_size)\n",
        "\n",
        "    # Append datasets to the list\n",
        "    train_datasets.append(train_dataset)\n",
        "    val_datasets.append(val_dataset)\n",
        "\n",
        "# Create a TensorFlow Dataset for the test dataset\n",
        "test_features = df_test.drop(columns=['labels']).values.astype(np.float32)\n",
        "test_labels = df_test['labels'].values.astype(np.int32)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels)).batch(batch_size)\n",
        "\n",
        "# Now you have `train_datasets`, `val_datasets`, and `test_dataset` ready for TensorFlow\n"
      ],
      "metadata": {
        "id": "pDyxL0Yyhy81"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the neural network using the Keras API\n",
        "class Net(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Define the layers\n",
        "        self.fc1 = tf.keras.layers.Dense(128, activation='relu', input_shape=(40,))\n",
        "        self.fc2 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.fc3 = tf.keras.layers.Dense(5)  # Assuming 5 output classes\n",
        "\n",
        "    def call(self, x):\n",
        "        # Define the forward pass\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Device setup is automatic in TensorFlow, so no need to explicitly specify DEVICE.\n",
        "\n",
        "def train(net, dataset, epochs, verbose=False):\n",
        "    net.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        history = net.fit(dataset, epochs=1, verbose=verbose)\n",
        "        total_loss += sum(history.history['loss'])\n",
        "        accuracy = history.history['accuracy'][-1]\n",
        "\n",
        "    return total_loss, accuracy\n",
        "\n",
        "def test(net, dataset):\n",
        "    results = net.evaluate(dataset, verbose=0)\n",
        "    return results[0], results[1]  # loss, accuracy\n",
        "\n",
        "net = Net()\n",
        "\n",
        "num_clients = 10\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Training for epoch {epoch+1}\")\n",
        "    epoch_training_loss = 0.0\n",
        "    epoch_training_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        dataloader = train_datasets[i]\n",
        "        valloader = val_datasets[i]\n",
        "\n",
        "        # Train on client data\n",
        "        train_loss, train_accuracy = train(net, dataloader, 1)\n",
        "        epoch_training_loss += train_loss * len(dataloader)\n",
        "        epoch_training_correct += train_accuracy * len(dataloader)\n",
        "        total_samples += len(dataloader)\n",
        "\n",
        "        # Validate on client data\n",
        "        val_loss, val_accuracy = test(net, valloader)\n",
        "        print(f\"Client {i+1} Epoch {epoch+1}: validation loss {val_loss}, accuracy {val_accuracy}\")\n",
        "\n",
        "    # Compute average training loss and accuracy for the epoch\n",
        "    average_epoch_loss = epoch_training_loss / total_samples\n",
        "    average_epoch_accuracy = epoch_training_correct / total_samples\n",
        "    print(f\"Epoch {epoch+1} Average Training Loss: {average_epoch_loss:.4f}, \"\n",
        "          f\"Average Training Accuracy: {average_epoch_accuracy:.4f}\")\n",
        "    print(f\"End of epoch {epoch+1}\\n\")\n",
        "\n",
        "# Test the model with the test dataset\n",
        "loss, accuracy = test(net, test_dataset)\n",
        "print(f\"Final test set performance:\\n\\tloss {loss}\\n\\taccuracy {accuracy}\")\n"
      ],
      "metadata": {
        "id": "8bwf36OImHgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-federated"
      ],
      "metadata": {
        "id": "YkP4z3slrLD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import collections\n",
        "\n",
        "# Define the Keras model\n",
        "def create_keras_model():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(40,)),  # Assuming 40 features\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')  # Assuming 5 output classes\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "nPA5Jqrh_JBJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function to handle batching with padding\n",
        "def preprocess(dataset):\n",
        "    def batch_format_fn(features, labels):\n",
        "        return collections.OrderedDict(\n",
        "            x=tf.reshape(features, [-1, 40]),  # Flatten features to match input shape\n",
        "            y=tf.reshape(labels, [-1])  # Reshape labels to match expected shape\n",
        "        )\n",
        "\n",
        "    # Pad and batch datasets\n",
        "    padded_shapes = ([None, 40], [None])  # Correct padded shapes for features and labels\n",
        "    return dataset.padded_batch(32, padded_shapes=padded_shapes).map(batch_format_fn).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Bd_Fgg7A_K-W"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the entire test dataset once\n",
        "preprocessed_test_data = preprocess(test_dataset)"
      ],
      "metadata": {
        "id": "TwM6HOplAA9g"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create federated data\n",
        "# def make_federated_data(train_dataset, client_ids):\n",
        "#     federated_data = []\n",
        "#     for i in client_ids:\n",
        "#         dataset_size = len(list(train_dataset[i]))  # Check dataset size\n",
        "#         if dataset_size > 0:  # Ensure the client has data\n",
        "#             federated_data.append(preprocess(train_dataset[i]))\n",
        "#     return federated_data\n",
        "def make_federated_data(train_dataset, client_ids):\n",
        "    federated_data = []\n",
        "    for i in client_ids:\n",
        "        client_data = list(train_dataset[i].as_numpy_iterator())  # Convert to list\n",
        "        dataset_size = len(client_data)  # Check dataset size\n",
        "        if dataset_size > 0:  # Ensure the client has data\n",
        "            federated_data.append(preprocess(client_data))\n",
        "    return federated_data\n"
      ],
      "metadata": {
        "id": "zWMmJn9K_UJK"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of clients\n",
        "NUM_CLIENTS = 10\n",
        "client_ids = list(range(NUM_CLIENTS))\n",
        "\n",
        "# Create federated training data\n",
        "federated_train_data = make_federated_data(train_datasets, client_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "wZ0-kmuT_Zp0",
        "outputId": "29259169-35a3-451a-f155-b15a804e1abb"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'padded_batch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-ab46d0f9f9b0>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create federated training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfederated_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_federated_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-66-4432a8b83aa6>\u001b[0m in \u001b[0;36mmake_federated_data\u001b[0;34m(train_dataset, client_ids)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_data\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Check dataset size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Ensure the client has data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mfederated_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfederated_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-c94bb7e931df>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Pad and batch datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpadded_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Correct padded shapes for features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadded_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadded_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_format_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'padded_batch'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model function for federated learning\n",
        "def model_fn():\n",
        "    keras_model = create_keras_model()\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model=keras_model,\n",
        "        input_spec=federated_train_data[0].element_spec,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n"
      ],
      "metadata": {
        "id": "KkpJMw86_fGS"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the federated averaging algorithm\n",
        "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.001),\n",
        "    server_optimizer_fn=tff.learning.optimizers.build_adam(learning_rate=0.01)\n",
        ")\n",
        "\n",
        "# Initialize the federated learning process\n",
        "train_state = training_process.initialize()\n"
      ],
      "metadata": {
        "id": "i807kpmK_kK8"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of training rounds\n",
        "NUM_ROUNDS = 15\n",
        "\n",
        "# Training loop for federated learning\n",
        "for round_num in range(NUM_ROUNDS):\n",
        "    # Select clients for this round (all clients in this case)\n",
        "    selected_clients = client_ids\n",
        "\n",
        "    # # Create federated data for the selected clients\n",
        "    # federated_train_data = make_federated_data(train_datasets, selected_clients)\n",
        "\n",
        "    # Run one round of federated training\n",
        "    result = training_process.next(train_state, federated_train_data)\n",
        "    train_state = result.state\n",
        "\n",
        "    # Extract and display the metrics\n",
        "    train_metrics = result.metrics\n",
        "    print(f'Round {round_num+1}, Metrics: {train_metrics}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM8vSlNO_o_0",
        "outputId": "71c3ac3c-10ca-4618-c491-f53a3e116e68"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.46319795), ('loss', 2.1830447), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 2, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.75371087), ('loss', 0.6949001), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 3, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.7717016), ('loss', 0.67965484), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 4, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.82366836), ('loss', 0.56637394), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 5, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.8410882), ('loss', 0.51045394), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 6, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.83694905), ('loss', 0.47524095), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 7, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.8650305), ('loss', 0.38443843), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 8, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.8949318), ('loss', 0.3290119), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 9, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.89957756), ('loss', 0.31388023), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 10, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.9028031), ('loss', 0.30721825), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 11, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.91459954), ('loss', 0.28210974), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 12, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.927766), ('loss', 0.24326637), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 13, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.9353662), ('loss', 0.22154588), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 14, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.9372288), ('loss', 0.21317534), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 15, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('sparse_categorical_accuracy', 0.9407756), ('loss', 0.20395483), ('num_examples', 140128), ('num_batches', 140)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the training has already been completed and train_state contains the global model weights\n",
        "# Extract global model weights after training\n",
        "global_model_weights = training_process.get_model_weights(train_state)\n",
        "\n",
        "# Prepare test federated data from distinct held-out clients\n",
        "federated_test_data = make_federated_data(test_dataset, selected_clients)  # Use real test clients here\n",
        "\n",
        "\n",
        "# Build the evaluation process for federated evaluation\n",
        "evaluation_process = tff.learning.algorithms.build_fed_eval(model_fn)\n",
        "\n",
        "# Initialize the evaluation state\n",
        "evaluation_state = evaluation_process.initialize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "BNxwavUsAGlM",
        "outputId": "280045ec-cffd-491f-e815-18fcbce12080"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'_BatchDataset' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-9bfc5b81c3ae>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Prepare test federated data from distinct held-out clients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfederated_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_federated_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_clients\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use real test clients here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-90fadbec7ef1>\u001b[0m in \u001b[0;36mmake_federated_data\u001b[0;34m(dataset, client_ids)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfederated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Check dataset size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Ensure the client has data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mfederated_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '_BatchDataset' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, use the global model weights (from the trained model) and evaluate on test data\n",
        "evaluation_state = tff.learning.state_with_new_model_weights(\n",
        "    evaluation_state, trainable=global_model_weights.trainable, non_trainable=global_model_weights.non_trainable\n",
        ")\n",
        "\n",
        "# Evaluate on the test federated data\n",
        "evaluation_result = evaluation_process.next(evaluation_state, federated_test_data)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Global model evaluation metrics on test dataset:\", evaluation_result.metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxQzrUf2AIi2",
        "outputId": "0819facf-d611-40cc-c577-86bd3c2dda7f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global model evaluation metrics on test dataset: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('eval', OrderedDict([('current_round_metrics', OrderedDict([('sparse_categorical_accuracy', 0.0019960965), ('loss', 4.1579432), ('num_examples', 22544), ('num_batches', 705)])), ('total_rounds_metrics', OrderedDict([('sparse_categorical_accuracy', 0.0019960965), ('loss', 4.1579432), ('num_examples', 22544), ('num_batches', 705)]))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
          ]
        }
      ]
    }
  ]
}